{"nbformat":4,"nbformat_minor":5,"metadata":{"environment":{"name":"tf2-gpu.2-6.m79","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"TRAC2-BERT-Models_augm_backt_mr_new.ipynb","provenance":[{"file_id":"1XNj1lgryIJJZHKGjnZUJIfZP7oO2LgVG","timestamp":1634778449393},{"file_id":"1mRQxrA8-WuG9cN_nqkzpAY1ax1IjnF5b","timestamp":1634696505317},{"file_id":"1d4fPHY04vX7cY8qIfTGKp51lmnQJyPlP","timestamp":1634070044259},{"file_id":"1eHLOgeKwtQWSLPIZwkDLEnrg20vAaHUa","timestamp":1634053840331}],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2672f05a4890471c969f5347e1e37c23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4a6d2a4397e9453dacc8d9a2ed8a993f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d426f3ed4324e90ad03bd84b8ebece6","IPY_MODEL_abc41175eaf74af08a90bbf2f0234a22","IPY_MODEL_d2de1e2d61bd46d79488098a6c2097aa"]}},"4a6d2a4397e9453dacc8d9a2ed8a993f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d426f3ed4324e90ad03bd84b8ebece6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_22ffd186407c44ac8718520178d8cfe5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f682dcff464c4fcbaf57a65396f89f4f"}},"abc41175eaf74af08a90bbf2f0234a22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d012df298a7f4451b2db30f79ffadedb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":32,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":32,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3727a2a659c4d11bbbc09c44355f93c"}},"d2de1e2d61bd46d79488098a6c2097aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b41ac9415afb42618f3a6cc169502f08","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32/32 [00:03&lt;00:00,  8.79ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60c2e3c8ebf4428497f0ddc96251714d"}},"22ffd186407c44ac8718520178d8cfe5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f682dcff464c4fcbaf57a65396f89f4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d012df298a7f4451b2db30f79ffadedb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f3727a2a659c4d11bbbc09c44355f93c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b41ac9415afb42618f3a6cc169502f08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"60c2e3c8ebf4428497f0ddc96251714d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"8f01d8e3"},"source":["# Final Project"],"id":"8f01d8e3"},{"cell_type":"markdown","metadata":{"id":"67b65eac"},"source":["## TRAC2- Transformer Models (BERT) - base uncased - Evaluate variability\n","\n","The purpose of this notebook is to evaluate the variability in the results of the models for classification task-A and task-B.\n","\n","These models use the training data augmented using back translation. For more details about the augmentation process refer to the `TRAC2-Data_Augm_Back_Translation.ipynb` notebook."],"id":"67b65eac"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0HMV_5JH3iL","executionInfo":{"status":"ok","timestamp":1636403246323,"user_tz":480,"elapsed":384,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"77ce9902-c2c8-49bd-c03b-a35ad40ed28d"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"H0HMV_5JH3iL","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"fc82e530"},"source":["## Package imports"],"id":"fc82e530"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss6OEdHqIWyt","executionInfo":{"status":"ok","timestamp":1636403249779,"user_tz":480,"elapsed":2785,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"c5307fcc-cd5e-46c9-ae07-b49e687965e2"},"source":["!pip install transformers"],"id":"ss6OEdHqIWyt","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__N7lLxVIYs0","executionInfo":{"status":"ok","timestamp":1636403252263,"user_tz":480,"elapsed":2489,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"4bde4bdf-b52a-47a0-e9db-073c2756b287"},"source":["!pip install datasets"],"id":"__N7lLxVIYs0","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.15.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.1.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"c0ad9cf3","executionInfo":{"status":"ok","timestamp":1636403252264,"user_tz":480,"elapsed":12,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# This tells matplotlib not to try opening a new window for each plot.\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","from datasets import load_dataset\n","\n","from sklearn.preprocessing import label_binarize\n","from sklearn import metrics\n","\n","import statistics"],"id":"c0ad9cf3","execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1b0b9771"},"source":["## Helper functions"],"id":"1b0b9771"},{"cell_type":"code","metadata":{"id":"4a8eaacb","executionInfo":{"status":"ok","timestamp":1636403252264,"user_tz":480,"elapsed":9,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["def from_logits_to_labels(pred, task):\n","    '''\n","    Returns labels based on predicted logits on labels [CAG,NAG,OAG] for task A. Task B is binary, and 'GEN' represents \n","    the positive class.\n","    Parameters:\n","    pred: array with model prediction\n","    task: either 'A' or 'B'\n","    '''\n","    index_a = {0:'CAG', 1:'NAG', 2:'OAG'}\n","    index_b = {0:'GEN', 1:'NGEN'}\n","    \n","    if task == 'A':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_a.get)(highest_prob_class.astype(int))\n","        \n","    elif task == 'B':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_b.get)(highest_prob_class.astype(int))\n","    else:\n","        labels = []\n","        \n","    return labels  "],"id":"4a8eaacb","execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"505b0828","executionInfo":{"status":"ok","timestamp":1636403252265,"user_tz":480,"elapsed":9,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["def to_one_hot_labels(string_labels):\n","    '''\n","    Returns one-hot encoded labels from a multi-class label vector e.g. ['cat', 'dog', 'dog', 'lion', 'cat', ...] \n","    Parameters:\n","    string_labels: \n","    '''\n","    labels = pd.get_dummies(string_labels)\n","    labels = labels.to_numpy()\n","    \n","    return labels"],"id":"505b0828","execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf367348","executionInfo":{"status":"ok","timestamp":1636403252265,"user_tz":480,"elapsed":9,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# this is modified to get the prediction as parameter\n","# to avoid predicting again since inference takes time\n","def confusion_matrix_plot(pred_labels, true_labels, task, normalize=None):\n","    '''\n","    Returns a confusion matrix with a nice format.\n","    Parameters:\n","    pred_labels: predicted labels\n","    true_labels: true labels \n","    task: 'A' or 'B'\n","    normalize: if want to normalize the confusion matrix normalize='true'\n","    '''\n","    \n","    # Create a confusion matrix\n","    cm = metrics.confusion_matrix(true_labels, pred_labels, normalize=normalize)\n","    cm = np.around(cm, 2)\n","\n","    # Plot the confusion matrix\n","    if task == 'A':\n","        axis_labels = ['CAG', 'NAG', 'OAG']\n","    elif task == 'B':\n","        axis_labels = ['GEN', 'NGEN']\n","\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","\n","    # Create the ticks and labels\n","    ax.set_xticks(np.arange(len(axis_labels)))\n","    ax.set_yticks(np.arange(len(axis_labels)))\n","    ax.set_xticklabels(axis_labels)\n","    ax.set_yticklabels(axis_labels)\n","\n","    # Axis titles\n","    plt.ylabel('True label', size=12)\n","    plt.xlabel('Predicted label', size=12)\n","\n","    # Loop over data dimensions and create text annotations.\n","    for i in range(len(axis_labels)):\n","        for j in range(len(axis_labels)):\n","            text = ax.text(j, i, cm[i, j],ha=\"center\", va=\"center\", color=\"dimgrey\", size=12)\n","    \n","    ax.set_title(\"Confusion Matrix\", size=16, weight=\"bold\")\n","    fig.tight_layout()\n","    plt.show()\n"],"id":"cf367348","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"17c5651f","executionInfo":{"status":"ok","timestamp":1636403252265,"user_tz":480,"elapsed":8,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["def loss_accuracy_plots(training_history, xrange, task):\n","    '''\n","    Returns plots for loss and accuracy during the training process of a NN.\n","    Parameters:\n","    training_history: object that stores the training history of the NN (from model.fit(...))\n","    xrange: range in x axis\n","    task: string used for the title in the plot\n","    '''\n","    \n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n","    \n","    # loss plot\n","    ax1.plot(training_history.history['loss'], color='black')\n","    ax1.plot(training_history.history['val_loss'], color='blue')\n","    ax1.set_title('Training and validation loss Sub-Task ' + task)\n","    ax1.legend(['training', 'development'])\n","    ax1.grid(which='both')\n","    ax1.set_xticks(np.arange(0, xrange, 2))\n","    \n","    # accuracy plot\n","    ax2.plot(training_history.history['categorical_accuracy'], color='black')\n","    ax2.plot(training_history.history['val_categorical_accuracy'], color='blue')\n","    ax2.set_title('Training and validation acccuracy Sub_Task ' + task)\n","    ax2.legend(['training', 'development'])\n","    ax2.grid(which='both')\n","    ax2.set_xticks(np.arange(0, xrange, 2))\n","    plt.show()\n","    "],"id":"17c5651f","execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544f08b5"},"source":["## Load data\n","Load training, development and test datasets."],"id":"544f08b5"},{"cell_type":"code","metadata":{"id":"c7a92330","executionInfo":{"status":"ok","timestamp":1636403252743,"user_tz":480,"elapsed":486,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# Load labels using pandas dataframes\n","\n","train_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_BT_task_A.csv')['Sub-task A']\n","train_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_BT_task_B.csv')['Sub-task B']\n","dev_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task A']\n","dev_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task B']\n","test_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_a.csv')['Sub-task A']\n","test_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_b.csv')['Sub-task B']"],"id":"c7a92330","execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e837f450","executionInfo":{"status":"ok","timestamp":1636403255803,"user_tz":480,"elapsed":3063,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"fc3b443b-8d6b-4e08-d19d-39bd02cdc951"},"source":["# Load text data using Hugging Face datasets\n","# need to use the split argument even though we are not splitting. If not, data is loaded as DatasetDict\n","# to load as dataset need to include the split parameter\n","train_dataset_a = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_train_BT_task_A.csv', split = 'train[:40500]')\n","train_dataset_b = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_train_BT_task_B.csv', split = 'train[:31632]')\n","dev_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv', split = 'train[:1066]')\n","test_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/test/trac2_eng_test.csv', split = 'train[:1200]')"],"id":"e837f450","execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-5ba5308a8edf8c9c\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-5ba5308a8edf8c9c/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-9bf5f2f7162e7e9e\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-9bf5f2f7162e7e9e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-a64066573b62035d\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a64066573b62035d/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-d1136119f167d116\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-d1136119f167d116/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"]}]},{"cell_type":"markdown","metadata":{"id":"86e2c1cf"},"source":["## Encode labels"],"id":"86e2c1cf"},{"cell_type":"code","metadata":{"id":"2bc320b3","executionInfo":{"status":"ok","timestamp":1636403255803,"user_tz":480,"elapsed":6,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# encode labels Task A- [CAG,NAG,OAG]\n","train_labels_a_enc = to_one_hot_labels(train_labels_a)\n","dev_labels_a_enc = to_one_hot_labels(dev_labels_a)\n","test_labels_a_enc = to_one_hot_labels(test_labels_a)\n"],"id":"2bc320b3","execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCpGlFvZZ1ua","executionInfo":{"status":"ok","timestamp":1636403256038,"user_tz":480,"elapsed":239,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# encode labels Task B- [GEN, NGEN]\n","train_labels_b_enc = to_one_hot_labels(train_labels_b)\n","dev_labels_b_enc = to_one_hot_labels(dev_labels_b)\n","test_labels_b_enc = to_one_hot_labels(test_labels_b)"],"id":"jCpGlFvZZ1ua","execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5db58504"},"source":["## Prepare TensorFlow datasets for BERT"],"id":"5db58504"},{"cell_type":"code","metadata":{"id":"415583c1","executionInfo":{"status":"ok","timestamp":1636403256039,"user_tz":480,"elapsed":3,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# remove columns to leave only the column with the posts. Column 'Text'\n","train_dataset_a = train_dataset_a.remove_columns(['Sub-task A'])\n","train_dataset_b = train_dataset_b.remove_columns(['Sub-task B'])\n","dev_dataset = dev_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B'])\n","test_dataset = test_dataset.remove_columns('ID')"],"id":"415583c1","execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ead92c9","executionInfo":{"status":"ok","timestamp":1636403258542,"user_tz":480,"elapsed":2506,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# define a BERT tokenizer\n","# use the bert-based-uncased tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"id":"5ead92c9","execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"cd498279","colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["2672f05a4890471c969f5347e1e37c23","4a6d2a4397e9453dacc8d9a2ed8a993f","4d426f3ed4324e90ad03bd84b8ebece6","abc41175eaf74af08a90bbf2f0234a22","d2de1e2d61bd46d79488098a6c2097aa","22ffd186407c44ac8718520178d8cfe5","f682dcff464c4fcbaf57a65396f89f4f","d012df298a7f4451b2db30f79ffadedb","f3727a2a659c4d11bbbc09c44355f93c","b41ac9415afb42618f3a6cc169502f08","60c2e3c8ebf4428497f0ddc96251714d"]},"executionInfo":{"status":"ok","timestamp":1636403262240,"user_tz":480,"elapsed":3723,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"3df9644b-b30f-4f4a-d1c0-99fc73e4a46e"},"source":["# tokenize the train, development and test data\n","# Use a max sequence of 150 tokens. Based on EDA this is enough for majority of posts\n","\n","train_dataset_tok_a = train_dataset_a.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","train_dataset_tok_b = train_dataset_b.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","dev_dataset_tok = dev_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","test_dataset_tok = test_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)"],"id":"cd498279","execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-5ba5308a8edf8c9c/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-1d25c725a48bed19.arrow\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2672f05a4890471c969f5347e1e37c23","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/32 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-a64066573b62035d/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-2542ad2e6150c376.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d1136119f167d116/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-16ae1ea8c72ccc54.arrow\n"]}]},{"cell_type":"code","metadata":{"id":"0526c186","executionInfo":{"status":"ok","timestamp":1636403262240,"user_tz":480,"elapsed":6,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# now we can remove the column with the original post from the dataset. We are going to use the result of tokenization for modeling\n","train_dataset_tok_a = train_dataset_tok_a.remove_columns(['Text']).with_format('tensorflow')\n","train_dataset_tok_b = train_dataset_tok_b.remove_columns(['Text']).with_format('tensorflow')\n","dev_dataset_tok = dev_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","test_dataset_tok = test_dataset_tok.remove_columns(['Text']).with_format('tensorflow')"],"id":"0526c186","execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"778a3871","executionInfo":{"status":"ok","timestamp":1636403263315,"user_tz":480,"elapsed":1079,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# extract features from tokenizer output: 'input_ids', 'token_type_ids', 'attention_mask'\n","train_features_a = {x: train_dataset_tok_a[x] for x in tokenizer.model_input_names}\n","train_features_b = {x: train_dataset_tok_b[x] for x in tokenizer.model_input_names}\n","dev_features = {x: dev_dataset_tok[x] for x in tokenizer.model_input_names}\n","test_features = {x: test_dataset_tok[x] for x in tokenizer.model_input_names}"],"id":"778a3871","execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9445e5","executionInfo":{"status":"ok","timestamp":1636403263315,"user_tz":480,"elapsed":6,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":["# batch data\n","\n","batch_size = 16\n","buffer_a = len(train_dataset_tok_a)\n","buffer_b = len(train_dataset_tok_b)\n","\n","# Task A\n","train_tf_dataset_a = tf.data.Dataset.from_tensor_slices((train_features_a, train_labels_a_enc)).shuffle(buffer_a).batch(batch_size)\n","dev_tf_dataset_a = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_a_enc)).batch(batch_size)\n","test_tf_dataset_a = tf.data.Dataset.from_tensor_slices((test_features, test_labels_a_enc)).batch(batch_size)\n","\n","# Task B\n","train_tf_dataset_b = tf.data.Dataset.from_tensor_slices((train_features_b, train_labels_b_enc)).shuffle(buffer_b).batch(batch_size)\n","dev_tf_dataset_b = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_b_enc)).batch(batch_size)\n","test_tf_dataset_b = tf.data.Dataset.from_tensor_slices((test_features, test_labels_b_enc)).batch(batch_size)"],"id":"2c9445e5","execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b59c68de"},"source":["## Model Task A"],"id":"b59c68de"},{"cell_type":"code","metadata":{"id":"nc1v9RnfLBNg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636411984000,"user_tz":480,"elapsed":8720689,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"063afddd-6a5d-4a53-d2dd-95f069bdae48"},"source":["# initialize lists to keep statistics of all runs\n","f1_NAG = []\n","f1_CAG = []\n","f1_OAG = []\n","f1_macro = []\n","f1_weighted = []\n","accuracy = []\n","\n","# run 15 times the model to get an idea of variability\n","for i in range(5):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_A\n","  except:\n","    pass\n","  \n","  # define the model. Task A is a classification task with 3 labels\n","  BERT_model_A = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","\n","  # compile model\n","  BERT_model_A.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  # fit model\n","  training_history = BERT_model_A.fit(train_tf_dataset_a, validation_data=dev_tf_dataset_a, epochs=2)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_A.predict(test_features)[0]\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'A')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_a, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NAG.append(x['NAG']['f1-score'])\n","  f1_CAG.append(x['CAG']['f1-score'])\n","  f1_OAG.append(x['OAG']['f1-score'])\n","  f1_macro.append(x['macro avg']['f1-score'])\n","  f1_weighted.append(x['weighted avg']['f1-score'])\n","  accuracy.append(x['accuracy'])\n","\n","# calculate mean\n","f1_NAG_mean = round(statistics.mean(f1_NAG), 3)\n","f1_CAG_mean = round(statistics.mean(f1_CAG), 3)\n","f1_OAG_mean = round(statistics.mean(f1_OAG), 3)\n","f1_macro_mean = round(statistics.mean(f1_macro), 3)\n","f1_weighted_mean = round(statistics.mean(f1_weighted), 3)\n","accuracy_mean = round(statistics.mean(accuracy), 3)\n","\n","# calculate standard deviation\n","f1_NAG_std = round(statistics.stdev(f1_NAG), 3)\n","f1_CAG_std = round(statistics.stdev(f1_CAG), 3)\n","f1_OAG_std = round(statistics.stdev(f1_OAG), 3)\n","f1_macro_std = round(statistics.stdev(f1_macro), 3)\n","f1_weighted_std = round(statistics.stdev(f1_weighted), 3)\n","accuracy_std = round(statistics.stdev(accuracy), 3)\n","\n","print('Class NAG')\n","print(f'Mean f1-score = {f1_NAG_mean}')\n","print(f'Standard deviation f1-score = {f1_NAG_std}\\n')\n","\n","print('Class CAG')\n","print(f'Mean f1-score = {f1_CAG_mean}')\n","print(f'Standard deviation f1-score = {f1_CAG_std}\\n')\n","\n","print('Class OAG')\n","print(f'Mean f1-score = {f1_OAG_mean}')\n","print(f'Standard deviation f1-score = {f1_OAG_std}\\n')\n","\n","print('Class Macro')\n","print(f'Mean f1-score = {f1_macro_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_std}\\n')\n","\n","print('Class Weighted')\n","print(f'Mean f1-score = {f1_weighted_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_std}\\n')\n","\n","print('Accuracy')\n","print(f'Mean = {accuracy_mean}')\n","print(f'Standard deviation = {accuracy_std}\\n')"],"id":"nc1v9RnfLBNg","execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2532/2532 [==============================] - 875s 339ms/step - loss: 0.1982 - categorical_accuracy: 0.9261 - val_loss: 0.8913 - val_categorical_accuracy: 0.7992\n","Epoch 2/2\n","2532/2532 [==============================] - 856s 338ms/step - loss: 0.0371 - categorical_accuracy: 0.9881 - val_loss: 0.9826 - val_categorical_accuracy: 0.8030\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2532/2532 [==============================] - 870s 338ms/step - loss: 0.2000 - categorical_accuracy: 0.9260 - val_loss: 0.8803 - val_categorical_accuracy: 0.8077\n","Epoch 2/2\n","2532/2532 [==============================] - 856s 338ms/step - loss: 0.0371 - categorical_accuracy: 0.9889 - val_loss: 1.0008 - val_categorical_accuracy: 0.7852\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2532/2532 [==============================] - 872s 339ms/step - loss: 0.1947 - categorical_accuracy: 0.9281 - val_loss: 0.8316 - val_categorical_accuracy: 0.8077\n","Epoch 2/2\n","2532/2532 [==============================] - 855s 338ms/step - loss: 0.0374 - categorical_accuracy: 0.9885 - val_loss: 1.0253 - val_categorical_accuracy: 0.8096\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2532/2532 [==============================] - 870s 339ms/step - loss: 0.2068 - categorical_accuracy: 0.9234 - val_loss: 0.8886 - val_categorical_accuracy: 0.8021\n","Epoch 2/2\n","2532/2532 [==============================] - 855s 338ms/step - loss: 0.0379 - categorical_accuracy: 0.9886 - val_loss: 1.2168 - val_categorical_accuracy: 0.7749\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2532/2532 [==============================] - 870s 338ms/step - loss: 0.2045 - categorical_accuracy: 0.9232 - val_loss: 1.1159 - val_categorical_accuracy: 0.7720\n","Epoch 2/2\n","2532/2532 [==============================] - 854s 337ms/step - loss: 0.0355 - categorical_accuracy: 0.9897 - val_loss: 1.2014 - val_categorical_accuracy: 0.7692\n","---------------------------Iteration 4 ---------------------------\n","\n","Class NAG\n","Mean f1-score = 0.826\n","Standard deviation f1-score = 0.027\n","\n","Class CAG\n","Mean f1-score = 0.324\n","Standard deviation f1-score = 0.057\n","\n","Class OAG\n","Mean f1-score = 0.61\n","Standard deviation f1-score = 0.053\n","\n","Class Macro\n","Mean f1-score = 0.587\n","Standard deviation f1-score = 0.033\n","\n","Class Weighted\n","Mean f1-score = 0.681\n","Standard deviation f1-score = 0.03\n","\n","Accuracy\n","Mean = 0.706\n","Standard deviation = 0.024\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9142a3df"},"source":["## Model Task B"],"id":"9142a3df"},{"cell_type":"code","metadata":{"id":"qdvEEPmqN5E-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636422213534,"user_tz":480,"elapsed":10229553,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"0e91d9c8-2f22-490a-fee0-3ce1c611b387"},"source":["# initialize lists to keep statistics of all runs\n","f1_NGEN = []\n","f1_GEN = []\n","f1_macro_b = []\n","f1_weighted_b = []\n","accuracy_b = []\n","\n","for i in range(5):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_B\n","  except:\n","    pass\n","  \n","  # define the model. Task B is a classification task with 2 labels\n","  BERT_model_B = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","  # compile model\n","  BERT_model_B.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  \n","  # fit model\n","  training_history = BERT_model_B.fit(train_tf_dataset_b, validation_data=dev_tf_dataset_b, epochs=3)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_B.predict(test_features)[0]\n","\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'B')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_b, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NGEN.append(x['NGEN']['f1-score'])\n","  f1_GEN.append(x['GEN']['f1-score'])\n","  f1_macro_b.append(x['macro avg']['f1-score'])\n","  f1_weighted_b.append(x['weighted avg']['f1-score'])\n","  accuracy_b.append(x['accuracy'])\n","\n","# calculate mean\n","f1_NGEN_mean = round(statistics.mean(f1_NGEN), 3)\n","f1_GEN_mean = round(statistics.mean(f1_GEN), 3)\n","f1_macro_b_mean = round(statistics.mean(f1_macro_b), 3)\n","f1_weighted_b_mean = round(statistics.mean(f1_weighted_b), 3)\n","accuracy_b_mean = round(statistics.mean(accuracy_b), 3)\n","\n","# calculate standard deviation\n","f1_NGEN_std = round(statistics.stdev(f1_NGEN), 3)\n","f1_GEN_std = round(statistics.stdev(f1_GEN), 3)\n","f1_macro_b_std = round(statistics.stdev(f1_macro_b), 3)\n","f1_weighted_b_std = round(statistics.stdev(f1_weighted_b), 3)\n","accuracy_b_std = round(statistics.stdev(accuracy_b), 3)\n","\n","print('Class NGEN')\n","print(f'Mean f1-score = {f1_NGEN_mean}')\n","print(f'Standard deviation f1-score = {f1_NGEN_std}\\n')\n","\n","print('Class GEN')\n","print(f'Mean f1-score = {f1_GEN_mean}')\n","print(f'Standard deviation f1-score = {f1_GEN_std}\\n')\n","\n","print('Macro')\n","print(f'Mean f1-score = {f1_macro_b_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_b_std}\\n')\n","\n","print('Weighted')\n","print(f'Mean f1-score = {f1_weighted_b_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_b_std}\\n')\n","\n","print('Accuracy')\n","print(f'Mean = {accuracy_b_mean}')\n","print(f'Standard deviation = {accuracy_b_std}\\n')\n"],"id":"qdvEEPmqN5E-","execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1977/1977 [==============================] - 685s 339ms/step - loss: 0.0926 - categorical_accuracy: 0.9647 - val_loss: 0.3534 - val_categorical_accuracy: 0.9212\n","Epoch 2/3\n","1977/1977 [==============================] - 668s 338ms/step - loss: 0.0188 - categorical_accuracy: 0.9940 - val_loss: 0.2935 - val_categorical_accuracy: 0.9343\n","Epoch 3/3\n","1977/1977 [==============================] - 669s 338ms/step - loss: 0.0195 - categorical_accuracy: 0.9934 - val_loss: 0.2889 - val_categorical_accuracy: 0.9493\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1977/1977 [==============================] - 683s 339ms/step - loss: 0.0909 - categorical_accuracy: 0.9663 - val_loss: 0.3013 - val_categorical_accuracy: 0.9250\n","Epoch 2/3\n","1977/1977 [==============================] - 667s 337ms/step - loss: 0.0240 - categorical_accuracy: 0.9920 - val_loss: 0.2797 - val_categorical_accuracy: 0.9343\n","Epoch 3/3\n","1977/1977 [==============================] - 667s 337ms/step - loss: 0.0143 - categorical_accuracy: 0.9945 - val_loss: 0.4833 - val_categorical_accuracy: 0.9390\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1977/1977 [==============================] - 684s 339ms/step - loss: 0.0869 - categorical_accuracy: 0.9680 - val_loss: 0.2965 - val_categorical_accuracy: 0.9334\n","Epoch 2/3\n","1977/1977 [==============================] - 668s 338ms/step - loss: 0.0189 - categorical_accuracy: 0.9943 - val_loss: 0.2963 - val_categorical_accuracy: 0.9475\n","Epoch 3/3\n","1977/1977 [==============================] - 669s 338ms/step - loss: 0.0267 - categorical_accuracy: 0.9912 - val_loss: 0.4231 - val_categorical_accuracy: 0.9334\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1977/1977 [==============================] - 686s 340ms/step - loss: 0.0947 - categorical_accuracy: 0.9653 - val_loss: 0.3523 - val_categorical_accuracy: 0.9418\n","Epoch 2/3\n","1977/1977 [==============================] - 669s 338ms/step - loss: 0.0235 - categorical_accuracy: 0.9929 - val_loss: 0.5064 - val_categorical_accuracy: 0.8715\n","Epoch 3/3\n","1977/1977 [==============================] - 671s 339ms/step - loss: 0.0153 - categorical_accuracy: 0.9947 - val_loss: 0.4622 - val_categorical_accuracy: 0.9268\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1977/1977 [==============================] - 685s 339ms/step - loss: 0.0919 - categorical_accuracy: 0.9656 - val_loss: 0.3047 - val_categorical_accuracy: 0.9287\n","Epoch 2/3\n","1977/1977 [==============================] - 668s 338ms/step - loss: 0.0221 - categorical_accuracy: 0.9928 - val_loss: 0.3909 - val_categorical_accuracy: 0.9465\n","Epoch 3/3\n","1977/1977 [==============================] - 668s 338ms/step - loss: 0.0204 - categorical_accuracy: 0.9935 - val_loss: 0.4253 - val_categorical_accuracy: 0.9231\n","---------------------------Iteration 4 ---------------------------\n","\n","Class NGEN\n","Mean f1-score = 0.905\n","Standard deviation f1-score = 0.01\n","\n","Class GEN\n","Mean f1-score = 0.493\n","Standard deviation f1-score = 0.034\n","\n","Macro\n","Mean f1-score = 0.699\n","Standard deviation f1-score = 0.018\n","\n","Weighted\n","Mean f1-score = 0.845\n","Standard deviation f1-score = 0.01\n","\n","Accuracy\n","Mean = 0.84\n","Standard deviation = 0.014\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"efcfaf92"},"source":["## References\n","\n","- Pre-processing data: https://huggingface.co/transformers/preprocessing.html\n","\n","- Fine-tunning a pre-trained model: https://huggingface.co/transformers/training.html\n","\n","- BERT: https://huggingface.co/transformers/model_doc/bert.html\n"],"id":"efcfaf92"},{"cell_type":"code","metadata":{"id":"ecf7c642","executionInfo":{"status":"ok","timestamp":1636422213534,"user_tz":480,"elapsed":23,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}}},"source":[""],"id":"ecf7c642","execution_count":34,"outputs":[]}]}