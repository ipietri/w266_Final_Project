{"nbformat":4,"nbformat_minor":5,"metadata":{"environment":{"name":"tf2-gpu.2-6.m79","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"TRAC2-BERT-Models_augm_EDA_mr1.ipynb","provenance":[{"file_id":"1XNj1lgryIJJZHKGjnZUJIfZP7oO2LgVG","timestamp":1634778449393},{"file_id":"1mRQxrA8-WuG9cN_nqkzpAY1ax1IjnF5b","timestamp":1634696505317},{"file_id":"1d4fPHY04vX7cY8qIfTGKp51lmnQJyPlP","timestamp":1634070044259},{"file_id":"1eHLOgeKwtQWSLPIZwkDLEnrg20vAaHUa","timestamp":1634053840331}],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"108c15fc0ccc4a7099f03f4f7bf55cd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1339dc8554c04b2e8772fde4dd5fd255","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d454ae159b7446b39559fdef86889278","IPY_MODEL_2250049571ee4aae942d6551fcfed6b5","IPY_MODEL_1902b3938b394a99a6e73a832906cffe"]}},"1339dc8554c04b2e8772fde4dd5fd255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d454ae159b7446b39559fdef86889278":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e3726400eb864e3790ac927c43ca8360","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1030368ff5d84809aecd7323eadb280b"}},"2250049571ee4aae942d6551fcfed6b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_91ce2623fe584fe3b558da20b013a46b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33477199f15a4b078bfc8caefdc72df0"}},"1902b3938b394a99a6e73a832906cffe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1f5afe5eaf1a409d9c91f4ced64cbe4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10/10 [00:01&lt;00:00,  8.56ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6aff6d8a97034c56907e922c64c9bbb6"}},"e3726400eb864e3790ac927c43ca8360":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1030368ff5d84809aecd7323eadb280b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91ce2623fe584fe3b558da20b013a46b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"33477199f15a4b078bfc8caefdc72df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f5afe5eaf1a409d9c91f4ced64cbe4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6aff6d8a97034c56907e922c64c9bbb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27566ba0087045f39a66003f8b4396ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_43ca9d01034645109cf6ed7ec3057d6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14569d2839984125ad24ba6bbad97a75","IPY_MODEL_5fdb3623dc7c4311bb62c3212056cc70","IPY_MODEL_ccc07e7e6eb449dd8cb2d8c261984e8a"]}},"43ca9d01034645109cf6ed7ec3057d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14569d2839984125ad24ba6bbad97a75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e18105bc481e446a8ee208386eb35dd5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ff3788c39ee43ca8f4b8fc2e8de85e2"}},"5fdb3623dc7c4311bb62c3212056cc70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_11c8d9bf9ee245f89a6499af16478938","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c58079ed8b9748bf9441aba37d41ded4"}},"ccc07e7e6eb449dd8cb2d8c261984e8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d5766993d7a1488299e5667ee51305f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  6.00ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03fd959bd1a640d8a80771c4d5faf8f7"}},"e18105bc481e446a8ee208386eb35dd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6ff3788c39ee43ca8f4b8fc2e8de85e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11c8d9bf9ee245f89a6499af16478938":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c58079ed8b9748bf9441aba37d41ded4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5766993d7a1488299e5667ee51305f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"03fd959bd1a640d8a80771c4d5faf8f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf09aef27ab24db8baee4c67a16ce621":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2de964ec748a445fae8f109acf7bc5dc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b5037120d4c946c28807f5a4e9f434e6","IPY_MODEL_0ad546db72954da0b843a52d38b97a2d","IPY_MODEL_47dccfed4b424faa941b033835128ba6"]}},"2de964ec748a445fae8f109acf7bc5dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5037120d4c946c28807f5a4e9f434e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6ad0a1e7b32e41429720b491e3de9f71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d780fa0b982e4854b1d23e0e04321107"}},"0ad546db72954da0b843a52d38b97a2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c2415bb544434f86b9966fd830fb1c3d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_58f760fc440e4d049fcb4f6001236a97"}},"47dccfed4b424faa941b033835128ba6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_081b207d985b4a2bb6919762578b94c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  4.11ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_568bcf4837ca417e9a1d204e57826165"}},"6ad0a1e7b32e41429720b491e3de9f71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d780fa0b982e4854b1d23e0e04321107":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2415bb544434f86b9966fd830fb1c3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"58f760fc440e4d049fcb4f6001236a97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"081b207d985b4a2bb6919762578b94c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"568bcf4837ca417e9a1d204e57826165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8f01d8e3"},"source":["# Final Project"],"id":"8f01d8e3"},{"cell_type":"markdown","metadata":{"id":"67b65eac"},"source":["## TRAC2- Transformer Models (BERT) - base uncased - Evaluate variability\n","\n","The purpose of this notebook is to evaluate the variability in the results of the models for classification task-A and task-B.\n","\n","These models use the training data augmented using Easy Data Augmentation (EDA) techniques. For more details about the augmentation process refer to the `TRAC2-Data_Augm_EDA.ipynb` notebook."],"id":"67b65eac"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0HMV_5JH3iL","executionInfo":{"status":"ok","timestamp":1634868035523,"user_tz":420,"elapsed":253,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"7599fc21-ccf9-40e5-ce47-ffa5ee8d3430"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"H0HMV_5JH3iL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"fc82e530"},"source":["## Package imports"],"id":"fc82e530"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss6OEdHqIWyt","executionInfo":{"status":"ok","timestamp":1634868039288,"user_tz":420,"elapsed":3525,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"66df27f4-5565-4afc-88f9-9d774be3421f"},"source":["!pip install transformers"],"id":"ss6OEdHqIWyt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__N7lLxVIYs0","executionInfo":{"status":"ok","timestamp":1634868042750,"user_tz":420,"elapsed":3465,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"1b7bc10a-b73d-45af-9aad-1508fbf9be13"},"source":["!pip install datasets"],"id":"__N7lLxVIYs0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.14.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"c0ad9cf3"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# This tells matplotlib not to try opening a new window for each plot.\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","from datasets import load_dataset\n","\n","from sklearn.preprocessing import label_binarize\n","from sklearn import metrics\n","\n","import statistics"],"id":"c0ad9cf3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1b0b9771"},"source":["## Helper functions"],"id":"1b0b9771"},{"cell_type":"code","metadata":{"id":"4a8eaacb"},"source":["def from_logits_to_labels(pred, task):\n","    '''\n","    Returns labels based on predicted logits on labels [CAG,NAG,OAG] for task A. Task B is binary, and 'GEN' represents \n","    the positive class.\n","    Parameters:\n","    pred: array with model prediction\n","    task: either 'A' or 'B'\n","    '''\n","    index_a = {0:'CAG', 1:'NAG', 2:'OAG'}\n","    index_b = {0:'GEN', 1:'NGEN'}\n","    \n","    if task == 'A':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_a.get)(highest_prob_class.astype(int))\n","        \n","    elif task == 'B':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_b.get)(highest_prob_class.astype(int))\n","    else:\n","        labels = []\n","        \n","    return labels  "],"id":"4a8eaacb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"505b0828"},"source":["def to_one_hot_labels(string_labels):\n","    '''\n","    Returns one-hot encoded labels from a multi-class label vector e.g. ['cat', 'dog', 'dog', 'lion', 'cat', ...] \n","    Parameters:\n","    string_labels: \n","    '''\n","    labels = pd.get_dummies(string_labels)\n","    labels = labels.to_numpy()\n","    \n","    return labels"],"id":"505b0828","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf367348"},"source":["# this is modified to get the prediction as parameter\n","# to avoid predicting again since inference takes time\n","def confusion_matrix_plot(pred_labels, true_labels, task, normalize=None):\n","    '''\n","    Returns a confusion matrix with a nice format.\n","    Parameters:\n","    pred_labels: predicted labels\n","    true_labels: true labels \n","    task: 'A' or 'B'\n","    normalize: if want to normalize the confusion matrix normalize='true'\n","    '''\n","    \n","    # Create a confusion matrix\n","    cm = metrics.confusion_matrix(true_labels, pred_labels, normalize=normalize)\n","    cm = np.around(cm, 2)\n","\n","    # Plot the confusion matrix\n","    if task == 'A':\n","        axis_labels = ['CAG', 'NAG', 'OAG']\n","    elif task == 'B':\n","        axis_labels = ['GEN', 'NGEN']\n","\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","\n","    # Create the ticks and labels\n","    ax.set_xticks(np.arange(len(axis_labels)))\n","    ax.set_yticks(np.arange(len(axis_labels)))\n","    ax.set_xticklabels(axis_labels)\n","    ax.set_yticklabels(axis_labels)\n","\n","    # Axis titles\n","    plt.ylabel('True label', size=12)\n","    plt.xlabel('Predicted label', size=12)\n","\n","    # Loop over data dimensions and create text annotations.\n","    for i in range(len(axis_labels)):\n","        for j in range(len(axis_labels)):\n","            text = ax.text(j, i, cm[i, j],ha=\"center\", va=\"center\", color=\"dimgrey\", size=12)\n","    \n","    ax.set_title(\"Confusion Matrix\", size=16, weight=\"bold\")\n","    fig.tight_layout()\n","    plt.show()\n"],"id":"cf367348","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"17c5651f"},"source":["def loss_accuracy_plots(training_history, xrange, task):\n","    '''\n","    Returns plots for loss and accuracy during the training process of a NN.\n","    Parameters:\n","    training_history: object that stores the training history of the NN (from model.fit(...))\n","    xrange: range in x axis\n","    task: string used for the title in the plot\n","    '''\n","    \n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n","    \n","    # loss plot\n","    ax1.plot(training_history.history['loss'], color='black')\n","    ax1.plot(training_history.history['val_loss'], color='blue')\n","    ax1.set_title('Training and validation loss Sub-Task ' + task)\n","    ax1.legend(['training', 'development'])\n","    ax1.grid(which='both')\n","    ax1.set_xticks(np.arange(0, xrange, 2))\n","    \n","    # accuracy plot\n","    ax2.plot(training_history.history['categorical_accuracy'], color='black')\n","    ax2.plot(training_history.history['val_categorical_accuracy'], color='blue')\n","    ax2.set_title('Training and validation acccuracy Sub_Task ' + task)\n","    ax2.legend(['training', 'development'])\n","    ax2.grid(which='both')\n","    ax2.set_xticks(np.arange(0, xrange, 2))\n","    plt.show()\n","    "],"id":"17c5651f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544f08b5"},"source":["## Load data\n","Load training, development and test datasets."],"id":"544f08b5"},{"cell_type":"code","metadata":{"id":"c7a92330"},"source":["# Load labels using pandas dataframes\n","\n","train_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_EDA.csv')['Sub-task A']\n","train_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_EDA.csv')['Sub-task B']\n","dev_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task A']\n","dev_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task B']\n","test_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_a.csv')['Sub-task A']\n","test_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_b.csv')['Sub-task B']"],"id":"c7a92330","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e837f450","executionInfo":{"status":"ok","timestamp":1634868044996,"user_tz":420,"elapsed":2249,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"beaa8523-b89b-46b9-c4ac-a51c6b564c14"},"source":["# Load text data using Hugging Face datasets\n","# need to use the split argument even though we are not splitting. If not, data is loaded as DatasetDict\n","# to load as dataset need to include the split parameter\n","train_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_train_EDA.csv', split = 'train[:9373]')\n","dev_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv', split = 'train[:1066]')\n","test_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/test/trac2_eng_test.csv', split = 'train[:1200]')"],"id":"e837f450","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-8daf69b0dbc6c4dd\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-8daf69b0dbc6c4dd/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-98e2ce082b0629f1\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-98e2ce082b0629f1/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-0ba61447556c92da\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-0ba61447556c92da/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"]}]},{"cell_type":"markdown","metadata":{"id":"86e2c1cf"},"source":["## Encode labels"],"id":"86e2c1cf"},{"cell_type":"code","metadata":{"id":"2bc320b3"},"source":["# encode labels Task A- [CAG,NAG,OAG]\n","train_labels_a_enc = to_one_hot_labels(train_labels_a)\n","dev_labels_a_enc = to_one_hot_labels(dev_labels_a)\n","test_labels_a_enc = to_one_hot_labels(test_labels_a)\n"],"id":"2bc320b3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCpGlFvZZ1ua"},"source":["# encode labels Task B- [GEN, NGEN]\n","train_labels_b_enc = to_one_hot_labels(train_labels_b)\n","dev_labels_b_enc = to_one_hot_labels(dev_labels_b)\n","test_labels_b_enc = to_one_hot_labels(test_labels_b)"],"id":"jCpGlFvZZ1ua","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5db58504"},"source":["## Prepare TensorFlow datasets for BERT"],"id":"5db58504"},{"cell_type":"code","metadata":{"id":"415583c1"},"source":["# remove columns to leave only the column with the posts. Column 'Text'\n","train_dataset = train_dataset.remove_columns(['Sub-task B', 'Sub-task A'])\n","dev_dataset = dev_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B'])\n","test_dataset = test_dataset.remove_columns('ID')"],"id":"415583c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ead92c9"},"source":["# define a BERT tokenizer\n","# use the bert-based-uncased tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"id":"5ead92c9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["108c15fc0ccc4a7099f03f4f7bf55cd3","1339dc8554c04b2e8772fde4dd5fd255","d454ae159b7446b39559fdef86889278","2250049571ee4aae942d6551fcfed6b5","1902b3938b394a99a6e73a832906cffe","e3726400eb864e3790ac927c43ca8360","1030368ff5d84809aecd7323eadb280b","91ce2623fe584fe3b558da20b013a46b","33477199f15a4b078bfc8caefdc72df0","1f5afe5eaf1a409d9c91f4ced64cbe4a","6aff6d8a97034c56907e922c64c9bbb6","27566ba0087045f39a66003f8b4396ed","43ca9d01034645109cf6ed7ec3057d6c","14569d2839984125ad24ba6bbad97a75","5fdb3623dc7c4311bb62c3212056cc70","ccc07e7e6eb449dd8cb2d8c261984e8a","e18105bc481e446a8ee208386eb35dd5","6ff3788c39ee43ca8f4b8fc2e8de85e2","11c8d9bf9ee245f89a6499af16478938","c58079ed8b9748bf9441aba37d41ded4","d5766993d7a1488299e5667ee51305f9","03fd959bd1a640d8a80771c4d5faf8f7","bf09aef27ab24db8baee4c67a16ce621","2de964ec748a445fae8f109acf7bc5dc","b5037120d4c946c28807f5a4e9f434e6","0ad546db72954da0b843a52d38b97a2d","47dccfed4b424faa941b033835128ba6","6ad0a1e7b32e41429720b491e3de9f71","d780fa0b982e4854b1d23e0e04321107","c2415bb544434f86b9966fd830fb1c3d","58f760fc440e4d049fcb4f6001236a97","081b207d985b4a2bb6919762578b94c2","568bcf4837ca417e9a1d204e57826165"]},"id":"cd498279","executionInfo":{"status":"ok","timestamp":1634868049239,"user_tz":420,"elapsed":1565,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"733be9e3-78a1-486e-9796-899161bea307"},"source":["# tokenize the train, development and test data\n","# Use a max sequence of 150 tokens. Based on EDA this is enough for majority of posts\n","\n","train_dataset_tok = train_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","dev_dataset_tok = dev_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","test_dataset_tok = test_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)"],"id":"cd498279","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"108c15fc0ccc4a7099f03f4f7bf55cd3","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/10 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27566ba0087045f39a66003f8b4396ed","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf09aef27ab24db8baee4c67a16ce621","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"0526c186"},"source":["# now we can remove the column with the original post from the dataset. We are going to use the result of tokenization for modeling\n","train_dataset_tok = train_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","dev_dataset_tok = dev_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","test_dataset_tok = test_dataset_tok.remove_columns(['Text']).with_format('tensorflow')"],"id":"0526c186","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"778a3871"},"source":["# extract features from tokenizer output: 'input_ids', 'token_type_ids', 'attention_mask'\n","train_features = {x: train_dataset_tok[x] for x in tokenizer.model_input_names}\n","dev_features = {x: dev_dataset_tok[x] for x in tokenizer.model_input_names}\n","test_features = {x: test_dataset_tok[x] for x in tokenizer.model_input_names}"],"id":"778a3871","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9445e5"},"source":["# batch data\n","\n","batch_size = 16\n","buffer = len(train_dataset_tok)\n","\n","# Task A\n","train_tf_dataset_a = tf.data.Dataset.from_tensor_slices((train_features, train_labels_a_enc)).shuffle(buffer).batch(batch_size)\n","dev_tf_dataset_a = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_a_enc)).batch(batch_size)\n","test_tf_dataset_a = tf.data.Dataset.from_tensor_slices((test_features, test_labels_a_enc)).batch(batch_size)\n","\n","# Task B\n","train_tf_dataset_b = tf.data.Dataset.from_tensor_slices((train_features, train_labels_b_enc)).shuffle(buffer).batch(batch_size)\n","dev_tf_dataset_b = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_b_enc)).batch(batch_size)\n","test_tf_dataset_b = tf.data.Dataset.from_tensor_slices((test_features, test_labels_b_enc)).batch(batch_size)"],"id":"2c9445e5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b59c68de"},"source":["## Model Task A"],"id":"b59c68de"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nc1v9RnfLBNg","executionInfo":{"status":"ok","timestamp":1634870502414,"user_tz":420,"elapsed":2452904,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"d7e7b155-1a0e-4a5d-d1ac-05bee5aa141c"},"source":["# initialize lists to keep statistics of all runs\n","f1_NAG = []\n","f1_CAG = []\n","f1_OAG = []\n","f1_macro = []\n","f1_weighted = []\n","accuracy = []\n","\n","# run 15 times the model to get an idea of variability\n","for i in range(5):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_A\n","  except:\n","    pass\n","  \n","  # define the model. Task A is a classification task with 3 labels\n","  BERT_model_A = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","\n","  # compile model\n","  BERT_model_A.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  # fit model\n","  training_history = BERT_model_A.fit(train_tf_dataset_a, validation_data=dev_tf_dataset_a, epochs=2)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_A.predict(test_features)[0]\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'A')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_a, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NAG.append(x['NAG']['f1-score'])\n","  f1_CAG.append(x['CAG']['f1-score'])\n","  f1_OAG.append(x['OAG']['f1-score'])\n","  f1_macro.append(x['macro avg']['f1-score'])\n","  f1_weighted.append(x['weighted avg']['f1-score'])\n","  accuracy.append(x['accuracy'])\n","\n","# calculate mean\n","f1_NAG_mean = round(statistics.mean(f1_NAG), 3)\n","f1_CAG_mean = round(statistics.mean(f1_CAG), 3)\n","f1_OAG_mean = round(statistics.mean(f1_OAG), 3)\n","f1_macro_mean = round(statistics.mean(f1_macro), 3)\n","f1_weighted_mean = round(statistics.mean(f1_weighted), 3)\n","accuracy_mean = round(statistics.mean(accuracy), 3)\n","\n","# calculate standard deviation\n","f1_NAG_std = round(statistics.stdev(f1_NAG), 3)\n","f1_CAG_std = round(statistics.stdev(f1_CAG), 3)\n","f1_OAG_std = round(statistics.stdev(f1_OAG), 3)\n","f1_macro_std = round(statistics.stdev(f1_macro), 3)\n","f1_weighted_std = round(statistics.stdev(f1_weighted), 3)\n","accuracy_std = round(statistics.stdev(accuracy), 3)\n","\n","print('Class NAG')\n","print(f'Mean f1-score = {f1_NAG_mean}')\n","print(f'Standard deviation f1-score = {f1_NAG_std}\\n')\n","\n","print('Class CAG')\n","print(f'Mean f1-score = {f1_CAG_mean}')\n","print(f'Standard deviation f1-score = {f1_CAG_std}\\n')\n","\n","print('Class OAG')\n","print(f'Mean f1-score = {f1_OAG_mean}')\n","print(f'Standard deviation f1-score = {f1_OAG_std}\\n')\n","\n","print('Class Macro')\n","print(f'Mean f1-score = {f1_macro_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_std}\\n')\n","\n","print('Class Weighted')\n","print(f'Mean f1-score = {f1_weighted_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_std}\\n')\n","\n","print('Accuracy')\n","print(f'Mean = {accuracy_mean}')\n","print(f'Standard deviation = {accuracy_std}\\n')"],"id":"nc1v9RnfLBNg","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 229s 361ms/step - loss: 0.5450 - categorical_accuracy: 0.7771 - val_loss: 0.7002 - val_categorical_accuracy: 0.7852\n","Epoch 2/2\n","586/586 [==============================] - 208s 355ms/step - loss: 0.1358 - categorical_accuracy: 0.9538 - val_loss: 0.6339 - val_categorical_accuracy: 0.7861\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 227s 359ms/step - loss: 0.5608 - categorical_accuracy: 0.7701 - val_loss: 0.5875 - val_categorical_accuracy: 0.7842\n","Epoch 2/2\n","586/586 [==============================] - 208s 355ms/step - loss: 0.1520 - categorical_accuracy: 0.9486 - val_loss: 0.7021 - val_categorical_accuracy: 0.7702\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 228s 359ms/step - loss: 0.5314 - categorical_accuracy: 0.7762 - val_loss: 0.5618 - val_categorical_accuracy: 0.7936\n","Epoch 2/2\n","586/586 [==============================] - 208s 355ms/step - loss: 0.1327 - categorical_accuracy: 0.9526 - val_loss: 0.6891 - val_categorical_accuracy: 0.7955\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 227s 359ms/step - loss: 0.5488 - categorical_accuracy: 0.7645 - val_loss: 0.5910 - val_categorical_accuracy: 0.7946\n","Epoch 2/2\n","586/586 [==============================] - 208s 354ms/step - loss: 0.1289 - categorical_accuracy: 0.9544 - val_loss: 0.7514 - val_categorical_accuracy: 0.7749\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 228s 360ms/step - loss: 0.5307 - categorical_accuracy: 0.7786 - val_loss: 0.6007 - val_categorical_accuracy: 0.7674\n","Epoch 2/2\n","586/586 [==============================] - 208s 354ms/step - loss: 0.1328 - categorical_accuracy: 0.9527 - val_loss: 0.6735 - val_categorical_accuracy: 0.7711\n","---------------------------Iteration 4 ---------------------------\n","\n","Class NAG\n","Mean f1-score = 0.829\n","Standard deviation f1-score = 0.022\n","\n","Class CAG\n","Mean f1-score = 0.381\n","Standard deviation f1-score = 0.123\n","\n","Class OAG\n","Mean f1-score = 0.619\n","Standard deviation f1-score = 0.034\n","\n","Class Macro\n","Mean f1-score = 0.61\n","Standard deviation f1-score = 0.043\n","\n","Class Weighted\n","Mean f1-score = 0.695\n","Standard deviation f1-score = 0.032\n","\n","Accuracy\n","Mean = 0.717\n","Standard deviation = 0.019\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9142a3df"},"source":["## Model Task B"],"id":"9142a3df"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdvEEPmqN5E-","executionInfo":{"status":"ok","timestamp":1634867779000,"user_tz":420,"elapsed":3442455,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"27f6a69d-2770-4234-c246-fc9de153787d"},"source":["# initialize lists to keep statistics of all runs\n","f1_NGEN = []\n","f1_GEN = []\n","f1_macro_b = []\n","f1_weighted_b = []\n","accuracy_b = []\n","\n","for i in range(5):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_B\n","  except:\n","    pass\n","  \n","  # define the model. Task B is a classification task with 2 labels\n","  BERT_model_B = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","  # compile model\n","  BERT_model_B.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  \n","  # fit model\n","  training_history = BERT_model_B.fit(train_tf_dataset_b, validation_data=dev_tf_dataset_b, epochs=3)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_B.predict(test_features)[0]\n","\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'B')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_b, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NGEN.append(x['NGEN']['f1-score'])\n","  f1_GEN.append(x['GEN']['f1-score'])\n","  f1_macro_b.append(x['macro avg']['f1-score'])\n","  f1_weighted_b.append(x['weighted avg']['f1-score'])\n","  accuracy_b.append(x['accuracy'])\n","\n","# calculate mean\n","f1_NGEN_mean = round(statistics.mean(f1_NGEN), 3)\n","f1_GEN_mean = round(statistics.mean(f1_GEN), 3)\n","f1_macro_b_mean = round(statistics.mean(f1_macro_b), 3)\n","f1_weighted_b_mean = round(statistics.mean(f1_weighted_b), 3)\n","accuracy_b_mean = round(statistics.mean(accuracy_b), 3)\n","\n","# calculate standard deviation\n","f1_NGEN_std = round(statistics.stdev(f1_NGEN), 3)\n","f1_GEN_std = round(statistics.stdev(f1_GEN), 3)\n","f1_macro_b_std = round(statistics.stdev(f1_macro_b), 3)\n","f1_weighted_b_std = round(statistics.stdev(f1_weighted_b), 3)\n","accuracy_b_std = round(statistics.stdev(accuracy_b), 3)\n","\n","print('Class NGEN')\n","print(f'Mean f1-score = {f1_NGEN_mean}')\n","print(f'Standard deviation f1-score = {f1_NGEN_std}\\n')\n","\n","print('Class GEN')\n","print(f'Mean f1-score = {f1_GEN_mean}')\n","print(f'Standard deviation f1-score = {f1_GEN_std}\\n')\n","\n","print('Macro')\n","print(f'Mean f1-score = {f1_macro_b_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_b_std}\\n')\n","\n","print('Weighted')\n","print(f'Mean f1-score = {f1_weighted_b_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_b_std}\\n')\n","\n","print('Accuracy')\n","print(f'Mean = {accuracy_b_mean}')\n","print(f'Standard deviation = {accuracy_b_std}\\n')\n"],"id":"qdvEEPmqN5E-","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 226s 358ms/step - loss: 0.2080 - categorical_accuracy: 0.9197 - val_loss: 0.2241 - val_categorical_accuracy: 0.9118\n","Epoch 2/3\n","586/586 [==============================] - 207s 354ms/step - loss: 0.0404 - categorical_accuracy: 0.9879 - val_loss: 0.2847 - val_categorical_accuracy: 0.9428\n","Epoch 3/3\n","586/586 [==============================] - 208s 355ms/step - loss: 0.0266 - categorical_accuracy: 0.9923 - val_loss: 0.3194 - val_categorical_accuracy: 0.9475\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 226s 359ms/step - loss: 0.2406 - categorical_accuracy: 0.9108 - val_loss: 0.2022 - val_categorical_accuracy: 0.9390\n","Epoch 2/3\n","586/586 [==============================] - 209s 356ms/step - loss: 0.0596 - categorical_accuracy: 0.9827 - val_loss: 0.2411 - val_categorical_accuracy: 0.9353\n","Epoch 3/3\n","586/586 [==============================] - 209s 357ms/step - loss: 0.0319 - categorical_accuracy: 0.9882 - val_loss: 0.2913 - val_categorical_accuracy: 0.9381\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 228s 359ms/step - loss: 0.2375 - categorical_accuracy: 0.9090 - val_loss: 0.2370 - val_categorical_accuracy: 0.9259\n","Epoch 2/3\n","586/586 [==============================] - 207s 354ms/step - loss: 0.0530 - categorical_accuracy: 0.9837 - val_loss: 0.2495 - val_categorical_accuracy: 0.9409\n","Epoch 3/3\n","586/586 [==============================] - 208s 354ms/step - loss: 0.0252 - categorical_accuracy: 0.9923 - val_loss: 0.2920 - val_categorical_accuracy: 0.9475\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 226s 359ms/step - loss: 0.2334 - categorical_accuracy: 0.9121 - val_loss: 0.2414 - val_categorical_accuracy: 0.9400\n","Epoch 2/3\n","586/586 [==============================] - 208s 355ms/step - loss: 0.0528 - categorical_accuracy: 0.9813 - val_loss: 0.2830 - val_categorical_accuracy: 0.9409\n","Epoch 3/3\n","586/586 [==============================] - 210s 358ms/step - loss: 0.0250 - categorical_accuracy: 0.9923 - val_loss: 0.3039 - val_categorical_accuracy: 0.9465\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 230s 363ms/step - loss: 0.2102 - categorical_accuracy: 0.9216 - val_loss: 0.2264 - val_categorical_accuracy: 0.9522\n","Epoch 2/3\n","586/586 [==============================] - 210s 358ms/step - loss: 0.0450 - categorical_accuracy: 0.9861 - val_loss: 0.3207 - val_categorical_accuracy: 0.9475\n","Epoch 3/3\n","586/586 [==============================] - 208s 355ms/step - loss: 0.0275 - categorical_accuracy: 0.9916 - val_loss: 0.2932 - val_categorical_accuracy: 0.9437\n","---------------------------Iteration 4 ---------------------------\n","\n","Class NGEN\n","Mean f1-score = 0.915\n","Standard deviation f1-score = 0.014\n","\n","Class GEN\n","Mean f1-score = 0.478\n","Standard deviation f1-score = 0.035\n","\n","Macro\n","Mean f1-score = 0.696\n","Standard deviation f1-score = 0.021\n","\n","Weighted\n","Mean f1-score = 0.851\n","Standard deviation f1-score = 0.015\n","\n","Accuracy\n","Mean = 0.854\n","Standard deviation = 0.021\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"efcfaf92"},"source":["## References\n","\n","- Pre-processing data: https://huggingface.co/transformers/preprocessing.html\n","\n","- Fine-tunning a pre-trained model: https://huggingface.co/transformers/training.html\n","\n","- BERT: https://huggingface.co/transformers/model_doc/bert.html\n"],"id":"efcfaf92"},{"cell_type":"code","metadata":{"id":"ecf7c642"},"source":[""],"id":"ecf7c642","execution_count":null,"outputs":[]}]}