{"nbformat":4,"nbformat_minor":5,"metadata":{"environment":{"name":"tf2-gpu.2-6.m79","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"TRAC2-BERT-Models_Task_A_binary_mr.ipynb","provenance":[{"file_id":"1d4fPHY04vX7cY8qIfTGKp51lmnQJyPlP","timestamp":1634070044259},{"file_id":"1eHLOgeKwtQWSLPIZwkDLEnrg20vAaHUa","timestamp":1634053840331}],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"4fcc01fd1e844958a0e38cb1c4c6f85e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ae6e970c951c48d2a8f881b64ce8925b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_752916b884b347f493e0f10ce044d6e0","IPY_MODEL_3554216725b04e92abe249a3feb7e7e9","IPY_MODEL_18bc61f9dc83443e85f9b14aaebe7a5e"]}},"ae6e970c951c48d2a8f881b64ce8925b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"752916b884b347f493e0f10ce044d6e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_00fb4dc95fc4438ead130a30d1e52d41","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_26cb358a2e464a6d876cf4a3b4a025ad"}},"3554216725b04e92abe249a3feb7e7e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_401dd25b768e455eba1808ea97e9148e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_66322bc744524f6db71fbec71fa2a62e"}},"18bc61f9dc83443e85f9b14aaebe7a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_384c1f9060e847c3ab35954ad6857720","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  5.84ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b392523861a34755b687932d27bb6ab8"}},"00fb4dc95fc4438ead130a30d1e52d41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"26cb358a2e464a6d876cf4a3b4a025ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"401dd25b768e455eba1808ea97e9148e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"66322bc744524f6db71fbec71fa2a62e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"384c1f9060e847c3ab35954ad6857720":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b392523861a34755b687932d27bb6ab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8f01d8e3"},"source":["# Final Project"],"id":"8f01d8e3"},{"cell_type":"markdown","metadata":{"id":"67b65eac"},"source":["## TRAC2- Transformer Models (BERT) - base uncased - Task A binary classification Task\n","\n","The purpose of this notebook is to create a classification model for Task A with only two classes: Aggressive (AG) and Non-aggresive. The classes OAG and CAG are combined into class AG.\n","\n","The notebook `TRAC2-Data_2_classes_Task_A.ipynb` combines the classes and creates the dataset."],"id":"67b65eac"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0HMV_5JH3iL","executionInfo":{"status":"ok","timestamp":1634669135293,"user_tz":420,"elapsed":335,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"7503f0f3-891e-40b7-dfe4-646b8e7afb0d"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"H0HMV_5JH3iL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"fc82e530"},"source":["## Package imports"],"id":"fc82e530"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss6OEdHqIWyt","executionInfo":{"status":"ok","timestamp":1634669138524,"user_tz":420,"elapsed":2712,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"2cfa5aa2-1377-45d5-ae19-d4652e5ec7e4"},"source":["!pip install transformers"],"id":"ss6OEdHqIWyt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__N7lLxVIYs0","executionInfo":{"status":"ok","timestamp":1634669141372,"user_tz":420,"elapsed":2851,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"b8f4268d-3666-45ac-9ffd-bbbb1585deee"},"source":["!pip install datasets"],"id":"__N7lLxVIYs0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.14.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"c0ad9cf3"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# This tells matplotlib not to try opening a new window for each plot.\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","from datasets import load_dataset\n","\n","from sklearn.preprocessing import label_binarize\n","from sklearn import metrics\n","\n","import statistics"],"id":"c0ad9cf3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1b0b9771"},"source":["## Helper functions"],"id":"1b0b9771"},{"cell_type":"code","metadata":{"id":"4a8eaacb"},"source":["def from_logits_to_labels(pred, task):\n","    '''\n","    Returns labels based on predicted logits on labels [CAG,NAG,OAG] for task A. Task B is binary, and 'GEN' represents \n","    the positive class.\n","    Parameters:\n","    pred: array with model prediction\n","    task: either 'A' or 'B'\n","    '''\n","    index_a = {0:'AG', 1:'NAG'}\n","    index_b = {0:'GEN', 1:'NGEN'}\n","    \n","    if task == 'A':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_a.get)(highest_prob_class.astype(int))\n","        \n","    elif task == 'B':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_b.get)(highest_prob_class.astype(int))\n","    else:\n","        labels = []\n","        \n","    return labels  "],"id":"4a8eaacb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"505b0828"},"source":["def to_one_hot_labels(string_labels):\n","    '''\n","    Returns one-hot encoded labels from a multi-class label vector e.g. ['cat', 'dog', 'dog', 'lion', 'cat', ...] \n","    Parameters:\n","    string_labels: \n","    '''\n","    labels = pd.get_dummies(string_labels)\n","    labels = labels.to_numpy()\n","    \n","    return labels"],"id":"505b0828","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544f08b5"},"source":["## Load data\n","Load training, development and test datasets."],"id":"544f08b5"},{"cell_type":"code","metadata":{"id":"c7a92330"},"source":["# Load labels using pandas dataframes\n","\n","train_labels = pd.read_csv('drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/train.csv')['label_a']\n","dev_labels = pd.read_csv('drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/dev.csv')['label_a']\n","test_labels = pd.read_csv('drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/test.csv')['label_a']"],"id":"c7a92330","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e837f450","executionInfo":{"status":"ok","timestamp":1634669146047,"user_tz":420,"elapsed":2078,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"e4d09227-c8a4-48dc-9031-3d116ffd1628"},"source":["# Load text data using Hugging Face datasets\n","# need to use the split argument even though we are not splitting. If not, data is loaded as DatasetDict\n","# to load as dataset need to include the split parameter\n","train_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/train.csv', split = 'train[:4263]')\n","dev_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/dev.csv', split = 'train[:1066]')\n","test_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/pet_files/all_data_task_A_two_classes/test.csv', split = 'train[:1200]')"],"id":"e837f450","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-5f77e630ad205ebf\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-5f77e630ad205ebf/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-9f5c4d2d22d4cdf0\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-9f5c4d2d22d4cdf0/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n","Using custom data configuration default-6807051e4db05c0e\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-6807051e4db05c0e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"]}]},{"cell_type":"markdown","metadata":{"id":"86e2c1cf"},"source":["## Encode labels"],"id":"86e2c1cf"},{"cell_type":"code","metadata":{"id":"2bc320b3"},"source":["# encode labels Task A- [AG,NAG]\n","train_labels_enc = to_one_hot_labels(train_labels)\n","dev_labels_enc = to_one_hot_labels(dev_labels)\n","test_labels_enc = to_one_hot_labels(test_labels)\n"],"id":"2bc320b3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5db58504"},"source":["## Prepare TensorFlow datasets for BERT"],"id":"5db58504"},{"cell_type":"code","metadata":{"id":"415583c1"},"source":["# remove columns to leave only the column with the posts. Column 'Text'\n","train_dataset = train_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B', 'label_a'])\n","dev_dataset = dev_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B', 'label_a'])\n","test_dataset = test_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B', 'label_a'])"],"id":"415583c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ead92c9"},"source":["# define a BERT tokenizer\n","# use the bert-based-uncased tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"id":"5ead92c9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["4fcc01fd1e844958a0e38cb1c4c6f85e","ae6e970c951c48d2a8f881b64ce8925b","752916b884b347f493e0f10ce044d6e0","3554216725b04e92abe249a3feb7e7e9","18bc61f9dc83443e85f9b14aaebe7a5e","00fb4dc95fc4438ead130a30d1e52d41","26cb358a2e464a6d876cf4a3b4a025ad","401dd25b768e455eba1808ea97e9148e","66322bc744524f6db71fbec71fa2a62e","384c1f9060e847c3ab35954ad6857720","b392523861a34755b687932d27bb6ab8"]},"id":"cd498279","executionInfo":{"status":"ok","timestamp":1634669149173,"user_tz":420,"elapsed":26,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"c79f4de5-7b48-4b03-90fa-cc9bf0c6e42d"},"source":["# tokenize the train, development and test data\n","# Tried to increase the sequence max lenght but I get an error. It should be enough 150 given the results of the EDA.\n","\n","train_dataset_tok = train_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","dev_dataset_tok = dev_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","test_dataset_tok = test_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)"],"id":"cd498279","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-5f77e630ad205ebf/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-d8af7848e47fc82e.arrow\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fcc01fd1e844958a0e38cb1c4c6f85e","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-6807051e4db05c0e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-1b31fc717748ad01.arrow\n"]}]},{"cell_type":"code","metadata":{"id":"0526c186"},"source":["# now we can remove the column with the original post from the dataset. We are going to use the result of tokenization for modeling\n","train_dataset_tok = train_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","dev_dataset_tok = dev_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","test_dataset_tok = test_dataset_tok.remove_columns(['Text']).with_format('tensorflow')"],"id":"0526c186","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"778a3871"},"source":["# extract features from tokenizer output: 'input_ids', 'token_type_ids', 'attention_mask'\n","train_features = {x: train_dataset_tok[x] for x in tokenizer.model_input_names}\n","dev_features = {x: dev_dataset_tok[x] for x in tokenizer.model_input_names}\n","test_features = {x: test_dataset_tok[x] for x in tokenizer.model_input_names}"],"id":"778a3871","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9445e5"},"source":["# batch data\n","\n","batch_size = 16\n","buffer = len(train_dataset_tok)\n","\n","# Task A\n","train_tf_dataset_a = tf.data.Dataset.from_tensor_slices((train_features, train_labels_enc)).shuffle(buffer).batch(batch_size)\n","dev_tf_dataset_a = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_enc)).batch(batch_size)\n","test_tf_dataset_a = tf.data.Dataset.from_tensor_slices((test_features, test_labels_enc)).batch(batch_size)\n"],"id":"2c9445e5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b59c68de"},"source":["## Model Task A"],"id":"b59c68de"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nc1v9RnfLBNg","executionInfo":{"status":"ok","timestamp":1634672860220,"user_tz":420,"elapsed":2122405,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"455292de-d40f-488a-e3c1-ea15851ee40a"},"source":["# initialize lists to keep statistics of all runs\n","f1_NAG = []\n","f1_AG = []\n","f1_macro = []\n","f1_weighted = []\n","accuracy =[]\n","\n","# run 15 times the model to get an idea of variability\n","for i in range(15):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_A\n","  except:\n","    pass\n","  \n","  # define the model. Task A is a classification task with 3 labels\n","  BERT_model_A = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","  # compile model\n","  BERT_model_A.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  # fit model\n","  training_history = BERT_model_A.fit(train_tf_dataset_a, validation_data=dev_tf_dataset_a, epochs=2)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_A.predict(test_features)[0]\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'A')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NAG.append(x['NAG']['f1-score'])\n","  f1_AG.append(x['AG']['f1-score'])\n","  f1_macro.append(x['macro avg']['f1-score'])\n","  f1_weighted.append(x['weighted avg']['f1-score'])\n","  accuracy.append(x['accuracy'])\n","\n","# calculate mean\n","f1_NAG_mean = round(statistics.mean(f1_NAG), 3)\n","f1_AG_mean = round(statistics.mean(f1_AG), 3)\n","f1_macro_mean = round(statistics.mean(f1_macro), 3)\n","f1_weighted_mean = round(statistics.mean(f1_weighted), 3)\n","accuracy_mean = round(statistics.mean(accuracy), 3)\n","\n","# calculate standard deviation\n","f1_NAG_std = round(statistics.stdev(f1_NAG), 3)\n","f1_AG_std = round(statistics.stdev(f1_AG), 3)\n","f1_macro_std = round(statistics.stdev(f1_macro), 3)\n","f1_weighted_std = round(statistics.stdev(f1_weighted), 3)\n","accuracy_std = round(statistics.stdev(accuracy), 3)\n","\n","print('Class NAG')\n","print(f'Mean f1-score = {f1_NAG_mean}')\n","print(f'Standard deviation f1-score = {f1_NAG_std}\\n')\n","\n","print('Class AG')\n","print(f'Mean f1-score = {f1_AG_mean}')\n","print(f'Standard deviation f1-score = {f1_AG_std}\\n')\n","\n","print('Class Macro')\n","print(f'Mean f1-score = {f1_macro_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_std}\\n')\n","\n","print('Class Weighted')\n","print(f'Mean f1-score = {f1_weighted_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_std}\\n')\n","\n","print('Accuracy')\n","print(f'Mean = {accuracy_mean}')\n","print(f'Standard deviation = {accuracy_std}\\n')"],"id":"nc1v9RnfLBNg","execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 115s 372ms/step - loss: 0.3962 - categorical_accuracy: 0.8128 - val_loss: 0.3611 - val_categorical_accuracy: 0.8283\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2870 - categorical_accuracy: 0.8722 - val_loss: 0.3442 - val_categorical_accuracy: 0.8321\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 113s 374ms/step - loss: 0.3928 - categorical_accuracy: 0.8065 - val_loss: 0.3509 - val_categorical_accuracy: 0.8265\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2703 - categorical_accuracy: 0.8851 - val_loss: 0.3778 - val_categorical_accuracy: 0.8321\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 113s 373ms/step - loss: 0.3868 - categorical_accuracy: 0.8168 - val_loss: 0.3562 - val_categorical_accuracy: 0.7946\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2791 - categorical_accuracy: 0.8719 - val_loss: 0.3518 - val_categorical_accuracy: 0.8340\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 113s 373ms/step - loss: 0.4046 - categorical_accuracy: 0.7994 - val_loss: 0.3667 - val_categorical_accuracy: 0.8161\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2832 - categorical_accuracy: 0.8668 - val_loss: 0.3693 - val_categorical_accuracy: 0.8180\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 114s 375ms/step - loss: 0.4038 - categorical_accuracy: 0.8044 - val_loss: 0.3756 - val_categorical_accuracy: 0.8199\n","Epoch 2/2\n","267/267 [==============================] - 97s 365ms/step - loss: 0.2955 - categorical_accuracy: 0.8614 - val_loss: 0.3833 - val_categorical_accuracy: 0.8105\n","---------------------------Iteration 4 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.4084 - categorical_accuracy: 0.8027 - val_loss: 0.3456 - val_categorical_accuracy: 0.8274\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2894 - categorical_accuracy: 0.8719 - val_loss: 0.4003 - val_categorical_accuracy: 0.8227\n","---------------------------Iteration 5 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 113s 373ms/step - loss: 0.3953 - categorical_accuracy: 0.8074 - val_loss: 0.3596 - val_categorical_accuracy: 0.8246\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2752 - categorical_accuracy: 0.8754 - val_loss: 0.3569 - val_categorical_accuracy: 0.8433\n","---------------------------Iteration 6 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 374ms/step - loss: 0.4497 - categorical_accuracy: 0.7858 - val_loss: 0.3671 - val_categorical_accuracy: 0.8274\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.3238 - categorical_accuracy: 0.8496 - val_loss: 0.3661 - val_categorical_accuracy: 0.8274\n","---------------------------Iteration 7 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.3885 - categorical_accuracy: 0.8112 - val_loss: 0.3558 - val_categorical_accuracy: 0.8274\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2600 - categorical_accuracy: 0.8815 - val_loss: 0.3911 - val_categorical_accuracy: 0.8246\n","---------------------------Iteration 8 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.3915 - categorical_accuracy: 0.8088 - val_loss: 0.3645 - val_categorical_accuracy: 0.8227\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2645 - categorical_accuracy: 0.8804 - val_loss: 0.3736 - val_categorical_accuracy: 0.8236\n","---------------------------Iteration 9 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 375ms/step - loss: 0.4152 - categorical_accuracy: 0.7985 - val_loss: 0.3519 - val_categorical_accuracy: 0.8330\n","Epoch 2/2\n","267/267 [==============================] - 97s 365ms/step - loss: 0.2932 - categorical_accuracy: 0.8625 - val_loss: 0.3584 - val_categorical_accuracy: 0.8349\n","---------------------------Iteration 10 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.3895 - categorical_accuracy: 0.8037 - val_loss: 0.3528 - val_categorical_accuracy: 0.8265\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2613 - categorical_accuracy: 0.8839 - val_loss: 0.3662 - val_categorical_accuracy: 0.8208\n","---------------------------Iteration 11 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.3828 - categorical_accuracy: 0.8128 - val_loss: 0.3649 - val_categorical_accuracy: 0.8227\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2603 - categorical_accuracy: 0.8883 - val_loss: 0.3478 - val_categorical_accuracy: 0.8377\n","---------------------------Iteration 12 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 374ms/step - loss: 0.4264 - categorical_accuracy: 0.8032 - val_loss: 0.3620 - val_categorical_accuracy: 0.8255\n","Epoch 2/2\n","267/267 [==============================] - 97s 364ms/step - loss: 0.2926 - categorical_accuracy: 0.8745 - val_loss: 0.3585 - val_categorical_accuracy: 0.8302\n","---------------------------Iteration 13 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","267/267 [==============================] - 114s 373ms/step - loss: 0.4021 - categorical_accuracy: 0.8116 - val_loss: 0.3513 - val_categorical_accuracy: 0.8293\n","Epoch 2/2\n","267/267 [==============================] - 97s 365ms/step - loss: 0.3027 - categorical_accuracy: 0.8646 - val_loss: 0.3530 - val_categorical_accuracy: 0.8199\n","---------------------------Iteration 14 ---------------------------\n","\n","Class NAG\n","Mean f1-score = 0.867\n","Standard deviation f1-score = 0.035\n","\n","Class AG\n","Mean f1-score = 0.787\n","Standard deviation f1-score = 0.114\n","\n","Class Macro\n","Mean f1-score = 0.827\n","Standard deviation f1-score = 0.074\n","\n","Class Weighted\n","Mean f1-score = 0.833\n","Standard deviation f1-score = 0.068\n","\n","Accuracy\n","Mean = 0.838\n","Standard deviation = 0.057\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"efcfaf92"},"source":["## References\n","\n","- Pre-processing data: https://huggingface.co/transformers/preprocessing.html\n","\n","- Fine-tunning a pre-trained model: https://huggingface.co/transformers/training.html\n","\n","- BERT: https://huggingface.co/transformers/model_doc/bert.html\n"],"id":"efcfaf92"},{"cell_type":"code","metadata":{"id":"ecf7c642"},"source":[""],"id":"ecf7c642","execution_count":null,"outputs":[]}]}