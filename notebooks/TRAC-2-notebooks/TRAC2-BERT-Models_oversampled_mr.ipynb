{"nbformat":4,"nbformat_minor":5,"metadata":{"environment":{"name":"tf2-gpu.2-6.m79","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"TRAC2-BERT-Models_oversampled_mr.ipynb","provenance":[{"file_id":"1mRQxrA8-WuG9cN_nqkzpAY1ax1IjnF5b","timestamp":1634086280855},{"file_id":"1d4fPHY04vX7cY8qIfTGKp51lmnQJyPlP","timestamp":1634070044259},{"file_id":"1eHLOgeKwtQWSLPIZwkDLEnrg20vAaHUa","timestamp":1634053840331}],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"d68140e1cb694f0e9f3360e477705b0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6b1e02840604e72a4cd4d7dfb84a7bf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d13afc1277744a19491b620b18f264f","IPY_MODEL_817767b637164aba9cb872b44eec3556","IPY_MODEL_62e80e56a41e4dea8a15a3c1c5748f1d"]}},"d6b1e02840604e72a4cd4d7dfb84a7bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d13afc1277744a19491b620b18f264f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a10e9582cb9444789435a466544d8238","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84a616ed7ea64410b6ad18868861f39a"}},"817767b637164aba9cb872b44eec3556":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_941aa406a95d41299631a9b93fdc0caa","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a0f15cd1836e410cba1471c4cef0961c"}},"62e80e56a41e4dea8a15a3c1c5748f1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8313ff85e9a14458930856ea194ecd5b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00, 35.35it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a76ea3a50b554887b5465f59461131b9"}},"a10e9582cb9444789435a466544d8238":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"84a616ed7ea64410b6ad18868861f39a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"941aa406a95d41299631a9b93fdc0caa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a0f15cd1836e410cba1471c4cef0961c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8313ff85e9a14458930856ea194ecd5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a76ea3a50b554887b5465f59461131b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db341834f58b4a6386e607609491313c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_deddef94d76e44c28edf2a604edfdfb1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aba9c6b40c5e4baeb4720008049476ac","IPY_MODEL_cef620c1694448b4911750288126b4d5","IPY_MODEL_5f36b663d85a4c78bf02a1ca8bb27d59"]}},"deddef94d76e44c28edf2a604edfdfb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aba9c6b40c5e4baeb4720008049476ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5efd7dc45c5740aab5cb2bda0982d1ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0a75c6d04c54ec08bf2c2cc7b488bcc"}},"cef620c1694448b4911750288126b4d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_70d9fb0325c54332a83655eecf0de81f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec082452d4a14f1d9eb49ea6c0c66198"}},"5f36b663d85a4c78bf02a1ca8bb27d59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4cf03424d2014fd292d18fc7e190cdfb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00, 19.65it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fdba715846c54477a83d8d0cbb68cbbe"}},"5efd7dc45c5740aab5cb2bda0982d1ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b0a75c6d04c54ec08bf2c2cc7b488bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70d9fb0325c54332a83655eecf0de81f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ec082452d4a14f1d9eb49ea6c0c66198":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4cf03424d2014fd292d18fc7e190cdfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fdba715846c54477a83d8d0cbb68cbbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"03a0dd9cfe2b44669d908c52ac17ceeb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b1c37dde262842f89f19b7a8501825af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8f00d65661a54a94bf8bf716442ac4f9","IPY_MODEL_62572917d5374f2db8f721a45374e39f","IPY_MODEL_050d0bad07804565b1bafe5695091092"]}},"b1c37dde262842f89f19b7a8501825af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f00d65661a54a94bf8bf716442ac4f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fbae7c7e0e9e4fafb57692460940ad61","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4f1934b9309424184bc36cc9821d653"}},"62572917d5374f2db8f721a45374e39f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f2c41756fb8546f59fa3e5415316dee6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29e4c6a1d5b441ebaf4849c58497200b"}},"050d0bad07804565b1bafe5695091092":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_59abc15a98c342ceb74c97d38ba3796b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/? [00:00&lt;00:00,  9.78 tables/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59c3848f5c6146b6be9af81377b89191"}},"fbae7c7e0e9e4fafb57692460940ad61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4f1934b9309424184bc36cc9821d653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2c41756fb8546f59fa3e5415316dee6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"29e4c6a1d5b441ebaf4849c58497200b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59abc15a98c342ceb74c97d38ba3796b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"59c3848f5c6146b6be9af81377b89191":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f3df322ebd649bd8e0450aed93fbfc4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3b41c82c94644a0a94e2210fe956783","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0c412dac45234440ba8ed0a134f171b6","IPY_MODEL_d8d1b3fade3540d2b12a4cefa52d4bf2","IPY_MODEL_a334f328c93148bc9145e2e267d981df"]}},"b3b41c82c94644a0a94e2210fe956783":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c412dac45234440ba8ed0a134f171b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_765040baf87647289c478791b0f20ee3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f20149973ea34cb79b24092dc156092e"}},"d8d1b3fade3540d2b12a4cefa52d4bf2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b00eae5972d4d999622de688c7aa6e6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_032e4ce5cc094e96a98042b33d1f1b8c"}},"a334f328c93148bc9145e2e267d981df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2eae80ff67fe4ea79146188ffc36bfac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 882B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e302a9fa86a4bab88202bd9463257b0"}},"765040baf87647289c478791b0f20ee3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f20149973ea34cb79b24092dc156092e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b00eae5972d4d999622de688c7aa6e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"032e4ce5cc094e96a98042b33d1f1b8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2eae80ff67fe4ea79146188ffc36bfac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e302a9fa86a4bab88202bd9463257b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ed8d2735ee740f48728e9e163f7c9c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54451b0f70d54acaad8e0778130be50f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_be92f0e6ffa24a069290f20bc763bbf5","IPY_MODEL_08b724f17cdf4f53afd4298c316b6731","IPY_MODEL_7d9c04bb161a44ca89833cd2cd03450f"]}},"54451b0f70d54acaad8e0778130be50f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be92f0e6ffa24a069290f20bc763bbf5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1c5552348135418a987d86e62df9a914","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8409d8f356ac4801b1ac3a7fefbde6f9"}},"08b724f17cdf4f53afd4298c316b6731":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cf15544001e44402889c2173cdbe5e31","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":536063208,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":536063208,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b2787e30ed94287839cdd5a81129c67"}},"7d9c04bb161a44ca89833cd2cd03450f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e6cc1fd2b29f4ca593d6fab9a0a566c3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 511M/511M [00:10&lt;00:00, 56.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ee31d53996e48a1b22bd178591ceeff"}},"1c5552348135418a987d86e62df9a914":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8409d8f356ac4801b1ac3a7fefbde6f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf15544001e44402889c2173cdbe5e31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8b2787e30ed94287839cdd5a81129c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6cc1fd2b29f4ca593d6fab9a0a566c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9ee31d53996e48a1b22bd178591ceeff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"8f01d8e3"},"source":["# Final Project"],"id":"8f01d8e3"},{"cell_type":"markdown","metadata":{"id":"67b65eac"},"source":["## TRAC2- Transformer Models (BERT) - base uncased - Oversampled- Evaluate variability\n","\n","The purpose of this notebook is to evaluate the variance in the results (f1-score) of the models for classification task-A and task-B.\n","\n","These models use the oversampled training data (6x minority classes)."],"id":"67b65eac"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0HMV_5JH3iL","executionInfo":{"status":"ok","timestamp":1634087035003,"user_tz":420,"elapsed":435855,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"441e86b4-2d68-46f5-91c7-32dc708f7e17"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"H0HMV_5JH3iL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"fc82e530"},"source":["## Package imports"],"id":"fc82e530"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss6OEdHqIWyt","executionInfo":{"status":"ok","timestamp":1634087041295,"user_tz":420,"elapsed":6300,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"01e334a8-51ec-414a-a56d-4ab83525d605"},"source":["!pip install transformers"],"id":"ss6OEdHqIWyt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 12.9 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 67.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 77.4 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 77.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__N7lLxVIYs0","executionInfo":{"status":"ok","timestamp":1634087047218,"user_tz":420,"elapsed":5930,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"a12b9d1a-7645-446b-a4eb-9370916afa95"},"source":["!pip install datasets"],"id":"__N7lLxVIYs0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n","\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 35.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 35.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 133 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 153 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 163 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 174 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 184 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 194 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 266 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 270 kB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 70.9 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 72.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (5.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 73.3 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 64.3 MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, datasets\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.10.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.0\n"]}]},{"cell_type":"code","metadata":{"id":"c0ad9cf3"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# This tells matplotlib not to try opening a new window for each plot.\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","from datasets import load_dataset\n","\n","from sklearn.preprocessing import label_binarize\n","from sklearn import metrics\n","\n","import statistics"],"id":"c0ad9cf3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1b0b9771"},"source":["## Helper functions"],"id":"1b0b9771"},{"cell_type":"code","metadata":{"id":"4a8eaacb"},"source":["def from_logits_to_labels(pred, task):\n","    '''\n","    Returns labels based on predicted logits on labels [CAG,NAG,OAG] for task A. Task B is binary, and 'GEN' represents \n","    the positive class.\n","    Parameters:\n","    pred: array with model prediction\n","    task: either 'A' or 'B'\n","    '''\n","    index_a = {0:'CAG', 1:'NAG', 2:'OAG'}\n","    index_b = {0:'GEN', 1:'NGEN'}\n","    \n","    if task == 'A':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_a.get)(highest_prob_class.astype(int))\n","        \n","    elif task == 'B':\n","        highest_prob_class = np.argmax(pred, axis=1)\n","        labels = np.vectorize(index_b.get)(highest_prob_class.astype(int))\n","    else:\n","        labels = []\n","        \n","    return labels  "],"id":"4a8eaacb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"505b0828"},"source":["def to_one_hot_labels(string_labels):\n","    '''\n","    Returns one-hot encoded labels from a multi-class label vector e.g. ['cat', 'dog', 'dog', 'lion', 'cat', ...] \n","    Parameters:\n","    string_labels: \n","    '''\n","    labels = pd.get_dummies(string_labels)\n","    labels = labels.to_numpy()\n","    \n","    return labels"],"id":"505b0828","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf367348"},"source":["# this is modified to get the prediction as parameter\n","# to avoid predicting again since inference takes time\n","def confusion_matrix_plot(pred_labels, true_labels, task, normalize=None):\n","    '''\n","    Returns a confusion matrix with a nice format.\n","    Parameters:\n","    pred_labels: predicted labels\n","    true_labels: true labels \n","    task: 'A' or 'B'\n","    normalize: if want to normalize the confusion matrix normalize='true'\n","    '''\n","    \n","    # Create a confusion matrix\n","    cm = metrics.confusion_matrix(true_labels, pred_labels, normalize=normalize)\n","    cm = np.around(cm, 2)\n","\n","    # Plot the confusion matrix\n","    if task == 'A':\n","        axis_labels = ['CAG', 'NAG', 'OAG']\n","    elif task == 'B':\n","        axis_labels = ['GEN', 'NGEN']\n","\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","\n","    # Create the ticks and labels\n","    ax.set_xticks(np.arange(len(axis_labels)))\n","    ax.set_yticks(np.arange(len(axis_labels)))\n","    ax.set_xticklabels(axis_labels)\n","    ax.set_yticklabels(axis_labels)\n","\n","    # Axis titles\n","    plt.ylabel('True label', size=12)\n","    plt.xlabel('Predicted label', size=12)\n","\n","    # Loop over data dimensions and create text annotations.\n","    for i in range(len(axis_labels)):\n","        for j in range(len(axis_labels)):\n","            text = ax.text(j, i, cm[i, j],ha=\"center\", va=\"center\", color=\"dimgrey\", size=12)\n","    \n","    ax.set_title(\"Confusion Matrix\", size=16, weight=\"bold\")\n","    fig.tight_layout()\n","    plt.show()\n"],"id":"cf367348","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"17c5651f"},"source":["def loss_accuracy_plots(training_history, xrange, task):\n","    '''\n","    Returns plots for loss and accuracy during the training process of a NN.\n","    Parameters:\n","    training_history: object that stores the training history of the NN (from model.fit(...))\n","    xrange: range in x axis\n","    task: string used for the title in the plot\n","    '''\n","    \n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n","    \n","    # loss plot\n","    ax1.plot(training_history.history['loss'], color='black')\n","    ax1.plot(training_history.history['val_loss'], color='blue')\n","    ax1.set_title('Training and validation loss Sub-Task ' + task)\n","    ax1.legend(['training', 'development'])\n","    ax1.grid(which='both')\n","    ax1.set_xticks(np.arange(0, xrange, 2))\n","    \n","    # accuracy plot\n","    ax2.plot(training_history.history['categorical_accuracy'], color='black')\n","    ax2.plot(training_history.history['val_categorical_accuracy'], color='blue')\n","    ax2.set_title('Training and validation acccuracy Sub_Task ' + task)\n","    ax2.legend(['training', 'development'])\n","    ax2.grid(which='both')\n","    ax2.set_xticks(np.arange(0, xrange, 2))\n","    plt.show()\n","    "],"id":"17c5651f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544f08b5"},"source":["## Load data\n","Load training, development and test datasets."],"id":"544f08b5"},{"cell_type":"code","metadata":{"id":"c7a92330"},"source":["# Load labels using pandas dataframes\n","\n","train_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_oversampled.csv')['Sub-task A']\n","train_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_train_oversampled.csv')['Sub-task B']\n","dev_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task A']\n","dev_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv')['Sub-task B']\n","test_labels_a = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_a.csv')['Sub-task A']\n","test_labels_b = pd.read_csv('drive/MyDrive/w266/release-files/gold/trac2_eng_gold_b.csv')['Sub-task B']"],"id":"c7a92330","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":382,"referenced_widgets":["d68140e1cb694f0e9f3360e477705b0c","d6b1e02840604e72a4cd4d7dfb84a7bf","4d13afc1277744a19491b620b18f264f","817767b637164aba9cb872b44eec3556","62e80e56a41e4dea8a15a3c1c5748f1d","a10e9582cb9444789435a466544d8238","84a616ed7ea64410b6ad18868861f39a","941aa406a95d41299631a9b93fdc0caa","a0f15cd1836e410cba1471c4cef0961c","8313ff85e9a14458930856ea194ecd5b","a76ea3a50b554887b5465f59461131b9","db341834f58b4a6386e607609491313c","deddef94d76e44c28edf2a604edfdfb1","aba9c6b40c5e4baeb4720008049476ac","cef620c1694448b4911750288126b4d5","5f36b663d85a4c78bf02a1ca8bb27d59","5efd7dc45c5740aab5cb2bda0982d1ba","b0a75c6d04c54ec08bf2c2cc7b488bcc","70d9fb0325c54332a83655eecf0de81f","ec082452d4a14f1d9eb49ea6c0c66198","4cf03424d2014fd292d18fc7e190cdfb","fdba715846c54477a83d8d0cbb68cbbe","03a0dd9cfe2b44669d908c52ac17ceeb","b1c37dde262842f89f19b7a8501825af","8f00d65661a54a94bf8bf716442ac4f9","62572917d5374f2db8f721a45374e39f","050d0bad07804565b1bafe5695091092","fbae7c7e0e9e4fafb57692460940ad61","c4f1934b9309424184bc36cc9821d653","f2c41756fb8546f59fa3e5415316dee6","29e4c6a1d5b441ebaf4849c58497200b","59abc15a98c342ceb74c97d38ba3796b","59c3848f5c6146b6be9af81377b89191","629e8e59717041c284634ce872a1dab9","51add8a3131d4f74908ac12c5e2f20b4","a9f309dfc99240798d7575dfd7ca4c59","d507d48f060d4d5cbe70034bc3cf7156","a49ebcb0a67e467eacf63a66df478ef7","dda86d1fa59f4c3f84a18baafcf7ad23"]},"id":"e837f450","executionInfo":{"status":"ok","timestamp":1634087091931,"user_tz":420,"elapsed":26483,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"90ef3919-14b9-40c4-db47-b628e3383bad"},"source":["# Load text data using Hugging Face datasets\n","# need to use the split argument even though we are not splitting. If not, data is loaded as DatasetDict\n","# to load as dataset need to include the split parameter\n","train_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_train_oversampled.csv', split = 'train[:9373]')\n","dev_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/eng/trac2_eng_dev.csv', split = 'train[:1066]')\n","test_dataset = load_dataset('csv', data_files='drive/MyDrive/w266/release-files/test/trac2_eng_test.csv', split = 'train[:1200]')"],"id":"e837f450","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-9a263d73d21126b2\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9a263d73d21126b2/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d68140e1cb694f0e9f3360e477705b0c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db341834f58b4a6386e607609491313c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03a0dd9cfe2b44669d908c52ac17ceeb","version_minor":0,"version_major":2},"text/plain":["0 tables [00:00, ? tables/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9a263d73d21126b2/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"]},{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-8fe574042ae65a17\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-8fe574042ae65a17/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"629e8e59717041c284634ce872a1dab9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51add8a3131d4f74908ac12c5e2f20b4","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9f309dfc99240798d7575dfd7ca4c59","version_minor":0,"version_major":2},"text/plain":["0 tables [00:00, ? tables/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8fe574042ae65a17/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"]},{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-f4f28cfddb9ffd8b\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f4f28cfddb9ffd8b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d507d48f060d4d5cbe70034bc3cf7156","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a49ebcb0a67e467eacf63a66df478ef7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dda86d1fa59f4c3f84a18baafcf7ad23","version_minor":0,"version_major":2},"text/plain":["0 tables [00:00, ? tables/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f4f28cfddb9ffd8b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"]}]},{"cell_type":"markdown","metadata":{"id":"86e2c1cf"},"source":["## Encode labels"],"id":"86e2c1cf"},{"cell_type":"code","metadata":{"id":"2bc320b3"},"source":["# encode labels Task A- [CAG,NAG,OAG]\n","train_labels_a_enc = to_one_hot_labels(train_labels_a)\n","dev_labels_a_enc = to_one_hot_labels(dev_labels_a)\n","test_labels_a_enc = to_one_hot_labels(test_labels_a)\n"],"id":"2bc320b3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCpGlFvZZ1ua"},"source":["# encode labels Task B- [GEN, NGEN]\n","train_labels_b_enc = to_one_hot_labels(train_labels_b)\n","dev_labels_b_enc = to_one_hot_labels(dev_labels_b)\n","test_labels_b_enc = to_one_hot_labels(test_labels_b)"],"id":"jCpGlFvZZ1ua","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5db58504"},"source":["## Prepare TensorFlow datasets for BERT"],"id":"5db58504"},{"cell_type":"code","metadata":{"id":"415583c1"},"source":["# remove columns to leave only the column with the posts. Column 'Text'\n","train_dataset = train_dataset.remove_columns(['ID', 'Sub-task B', 'Sub-task A'])\n","dev_dataset = dev_dataset.remove_columns(['ID', 'Sub-task A', 'Sub-task B'])\n","test_dataset = test_dataset.remove_columns('ID')"],"id":"415583c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ead92c9","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["0f3df322ebd649bd8e0450aed93fbfc4","b3b41c82c94644a0a94e2210fe956783","0c412dac45234440ba8ed0a134f171b6","d8d1b3fade3540d2b12a4cefa52d4bf2","a334f328c93148bc9145e2e267d981df","765040baf87647289c478791b0f20ee3","f20149973ea34cb79b24092dc156092e","4b00eae5972d4d999622de688c7aa6e6","032e4ce5cc094e96a98042b33d1f1b8c","2eae80ff67fe4ea79146188ffc36bfac","6e302a9fa86a4bab88202bd9463257b0","743206480fe34e2babc28f777a491447","4848b74481dd4f75bf9e9efd676f6968","b9deda2394414cd382eef62454b0f4ad"]},"executionInfo":{"status":"ok","timestamp":1634087097297,"user_tz":420,"elapsed":5371,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"ac275537-42ea-47cb-e7b0-807e44174111"},"source":["# define a BERT tokenizer\n","# use the bert-based-uncased tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"id":"5ead92c9","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f3df322ebd649bd8e0450aed93fbfc4","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"743206480fe34e2babc28f777a491447","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4848b74481dd4f75bf9e9efd676f6968","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9deda2394414cd382eef62454b0f4ad","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["4040985d7c0c47f4aa1ec56e07e5fce8","ce4759b3d50f49908c2f8e4722afd6c5","17a8bb6f2baa472cbc54c8221417dc6c"]},"id":"cd498279","executionInfo":{"status":"ok","timestamp":1634087099337,"user_tz":420,"elapsed":2055,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"10c81d04-536d-45af-8a4e-7ffb6dede482"},"source":["# tokenize the train, development and test data\n","# Use a max sequence of 150 tokens. Based on EDA this is enough for majority of posts\n","\n","train_dataset_tok = train_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","dev_dataset_tok = dev_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)\n","test_dataset_tok = test_dataset.map(lambda x: tokenizer(x['Text'], truncation=True, padding=True, max_length=150), batched=True)"],"id":"cd498279","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4040985d7c0c47f4aa1ec56e07e5fce8","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/10 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce4759b3d50f49908c2f8e4722afd6c5","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17a8bb6f2baa472cbc54c8221417dc6c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"0526c186"},"source":["# now we can remove the column with the original post from the dataset. We are going to use the result of tokenization for modeling\n","train_dataset_tok = train_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","dev_dataset_tok = dev_dataset_tok.remove_columns(['Text']).with_format('tensorflow')\n","test_dataset_tok = test_dataset_tok.remove_columns(['Text']).with_format('tensorflow')"],"id":"0526c186","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"778a3871"},"source":["# extract features from tokenizer output: 'input_ids', 'token_type_ids', 'attention_mask'\n","train_features = {x: train_dataset_tok[x].to_tensor() for x in tokenizer.model_input_names}\n","dev_features = {x: dev_dataset_tok[x].to_tensor() for x in tokenizer.model_input_names}\n","test_features = {x: test_dataset_tok[x].to_tensor() for x in tokenizer.model_input_names}"],"id":"778a3871","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c9445e5"},"source":["# batch data\n","\n","batch_size = 16\n","buffer = len(train_dataset_tok)\n","\n","# Task A\n","train_tf_dataset_a = tf.data.Dataset.from_tensor_slices((train_features, train_labels_a_enc)).shuffle(buffer).batch(batch_size)\n","dev_tf_dataset_a = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_a_enc)).batch(batch_size)\n","test_tf_dataset_a = tf.data.Dataset.from_tensor_slices((test_features, test_labels_a_enc)).batch(batch_size)\n","\n","# Task B\n","train_tf_dataset_b = tf.data.Dataset.from_tensor_slices((train_features, train_labels_b_enc)).shuffle(buffer).batch(batch_size)\n","dev_tf_dataset_b = tf.data.Dataset.from_tensor_slices((dev_features, dev_labels_b_enc)).batch(batch_size)\n","test_tf_dataset_b = tf.data.Dataset.from_tensor_slices((test_features, test_labels_b_enc)).batch(batch_size)"],"id":"2c9445e5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b59c68de"},"source":["## Model Task A"],"id":"b59c68de"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7ed8d2735ee740f48728e9e163f7c9c8","54451b0f70d54acaad8e0778130be50f","be92f0e6ffa24a069290f20bc763bbf5","08b724f17cdf4f53afd4298c316b6731","7d9c04bb161a44ca89833cd2cd03450f","1c5552348135418a987d86e62df9a914","8409d8f356ac4801b1ac3a7fefbde6f9","cf15544001e44402889c2173cdbe5e31","8b2787e30ed94287839cdd5a81129c67","e6cc1fd2b29f4ca593d6fab9a0a566c3","9ee31d53996e48a1b22bd178591ceeff"]},"id":"nc1v9RnfLBNg","executionInfo":{"status":"ok","timestamp":1634094213971,"user_tz":420,"elapsed":3583195,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"5e50a597-857b-44d0-8832-cb4319a5ed3a"},"source":["# initialize lists to keep statistics of all runs\n","f1_NAG = []\n","f1_CAG = []\n","f1_OAG = []\n","f1_macro = []\n","f1_weighted = []\n","\n","# run 15 times the model to get an idea of variability\n","for i in range(15):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_A\n","  except:\n","    pass\n","  \n","  # define the model. Task A is a classification task with 3 labels\n","  BERT_model_A = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","\n","  # compile model\n","  BERT_model_A.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  # fit model\n","  training_history = BERT_model_A.fit(train_tf_dataset_a, validation_data=dev_tf_dataset_a, epochs=2)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_A.predict(test_features)[0]\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'A')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_a, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NAG.append(x['NAG']['f1-score'])\n","  f1_CAG.append(x['CAG']['f1-score'])\n","  f1_OAG.append(x['OAG']['f1-score'])\n","  f1_macro.append(x['macro avg']['f1-score'])\n","  f1_weighted.append(x['weighted avg']['f1-score'])\n","\n","# calculate mean\n","f1_NAG_mean = round(statistics.mean(f1_NAG), 3)\n","f1_CAG_mean = round(statistics.mean(f1_CAG), 3)\n","f1_OAG_mean = round(statistics.mean(f1_OAG), 3)\n","f1_macro_mean = round(statistics.mean(f1_macro), 3)\n","f1_weighted_mean = round(statistics.mean(f1_weighted), 3)\n","\n","# calculate standard deviation\n","f1_NAG_std = round(statistics.stdev(f1_NAG), 3)\n","f1_CAG_std = round(statistics.stdev(f1_CAG), 3)\n","f1_OAG_std = round(statistics.stdev(f1_OAG), 3)\n","f1_macro_std = round(statistics.stdev(f1_macro), 3)\n","f1_weighted_std = round(statistics.stdev(f1_weighted), 3)\n","\n","print('Class NAG')\n","print(f'Mean f1-score = {f1_NAG_mean}')\n","print(f'Standard deviation f1-score = {f1_NAG_std}\\n')\n","\n","print('Class CAG')\n","print(f'Mean f1-score = {f1_CAG_mean}')\n","print(f'Standard deviation f1-score = {f1_CAG_std}\\n')\n","\n","print('Class OAG')\n","print(f'Mean f1-score = {f1_OAG_mean}')\n","print(f'Standard deviation f1-score = {f1_OAG_std}\\n')\n","\n","print('Class Macro')\n","print(f'Mean f1-score = {f1_macro_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_std}\\n')\n","\n","print('Class Weighted')\n","print(f'Mean f1-score = {f1_weighted_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_std}\\n')"],"id":"nc1v9RnfLBNg","execution_count":null,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ed8d2735ee740f48728e9e163f7c9c8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 222s 352ms/step - loss: 0.4870 - categorical_accuracy: 0.7994 - val_loss: 0.6901 - val_categorical_accuracy: 0.7692\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0891 - categorical_accuracy: 0.9723 - val_loss: 0.7758 - val_categorical_accuracy: 0.8030\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 219s 352ms/step - loss: 0.4929 - categorical_accuracy: 0.8009 - val_loss: 0.6320 - val_categorical_accuracy: 0.7805\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.1062 - categorical_accuracy: 0.9659 - val_loss: 0.8590 - val_categorical_accuracy: 0.7720\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.4962 - categorical_accuracy: 0.7947 - val_loss: 0.6134 - val_categorical_accuracy: 0.8124\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.1002 - categorical_accuracy: 0.9674 - val_loss: 0.8060 - val_categorical_accuracy: 0.7992\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.4760 - categorical_accuracy: 0.8053 - val_loss: 0.6562 - val_categorical_accuracy: 0.8030\n","Epoch 2/2\n","586/586 [==============================] - 203s 347ms/step - loss: 0.0975 - categorical_accuracy: 0.9672 - val_loss: 0.9432 - val_categorical_accuracy: 0.7927\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 218s 350ms/step - loss: 0.5268 - categorical_accuracy: 0.7833 - val_loss: 0.6068 - val_categorical_accuracy: 0.8002\n","Epoch 2/2\n","586/586 [==============================] - 202s 345ms/step - loss: 0.1015 - categorical_accuracy: 0.9679 - val_loss: 0.8537 - val_categorical_accuracy: 0.7683\n","---------------------------Iteration 4 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 218s 350ms/step - loss: 0.4694 - categorical_accuracy: 0.8057 - val_loss: 0.6381 - val_categorical_accuracy: 0.8133\n","Epoch 2/2\n","586/586 [==============================] - 203s 346ms/step - loss: 0.0938 - categorical_accuracy: 0.9714 - val_loss: 0.7443 - val_categorical_accuracy: 0.8021\n","---------------------------Iteration 5 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","586/586 [==============================] - 218s 349ms/step - loss: 0.4814 - categorical_accuracy: 0.8051 - val_loss: 0.6390 - val_categorical_accuracy: 0.7786\n","Epoch 2/2\n","586/586 [==============================] - 202s 345ms/step - loss: 0.0998 - categorical_accuracy: 0.9687 - val_loss: 0.8330 - val_categorical_accuracy: 0.7946\n","---------------------------Iteration 6 ---------------------------\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 218s 350ms/step - loss: 0.4745 - categorical_accuracy: 0.8082 - val_loss: 0.6469 - val_categorical_accuracy: 0.7627\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0883 - categorical_accuracy: 0.9738 - val_loss: 0.7302 - val_categorical_accuracy: 0.8002\n","---------------------------Iteration 7 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 221s 353ms/step - loss: 0.4850 - categorical_accuracy: 0.8025 - val_loss: 0.5947 - val_categorical_accuracy: 0.7946\n","Epoch 2/2\n","586/586 [==============================] - 205s 349ms/step - loss: 0.0952 - categorical_accuracy: 0.9706 - val_loss: 0.8744 - val_categorical_accuracy: 0.7889\n","---------------------------Iteration 8 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.5258 - categorical_accuracy: 0.7832 - val_loss: 0.7948 - val_categorical_accuracy: 0.7195\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.1167 - categorical_accuracy: 0.9626 - val_loss: 0.7903 - val_categorical_accuracy: 0.8021\n","---------------------------Iteration 9 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.4967 - categorical_accuracy: 0.7956 - val_loss: 0.6905 - val_categorical_accuracy: 0.7739\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0934 - categorical_accuracy: 0.9717 - val_loss: 0.7775 - val_categorical_accuracy: 0.8030\n","---------------------------Iteration 10 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 221s 353ms/step - loss: 0.4791 - categorical_accuracy: 0.8091 - val_loss: 0.6113 - val_categorical_accuracy: 0.8086\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0978 - categorical_accuracy: 0.9692 - val_loss: 0.8300 - val_categorical_accuracy: 0.7871\n","---------------------------Iteration 11 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.4766 - categorical_accuracy: 0.8050 - val_loss: 0.6839 - val_categorical_accuracy: 0.7861\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0888 - categorical_accuracy: 0.9713 - val_loss: 0.8842 - val_categorical_accuracy: 0.7580\n","---------------------------Iteration 12 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 220s 352ms/step - loss: 0.4721 - categorical_accuracy: 0.8104 - val_loss: 0.6931 - val_categorical_accuracy: 0.7824\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0975 - categorical_accuracy: 0.9678 - val_loss: 0.9069 - val_categorical_accuracy: 0.7730\n","---------------------------Iteration 13 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","586/586 [==============================] - 221s 353ms/step - loss: 0.4885 - categorical_accuracy: 0.8009 - val_loss: 0.6482 - val_categorical_accuracy: 0.7692\n","Epoch 2/2\n","586/586 [==============================] - 204s 348ms/step - loss: 0.1004 - categorical_accuracy: 0.9687 - val_loss: 0.8185 - val_categorical_accuracy: 0.8068\n","---------------------------Iteration 14 ---------------------------\n","\n","Class NAG\n","Mean f1-score = 0.852\n","Standard deviation f1-score = 0.02\n","\n","Class CAG\n","Mean f1-score = 0.388\n","Standard deviation f1-score = 0.109\n","\n","Class OAG\n","Mean f1-score = 0.661\n","Standard deviation f1-score = 0.034\n","\n","Class Macro\n","Mean f1-score = 0.634\n","Standard deviation f1-score = 0.039\n","\n","Class Weighted\n","Mean f1-score = 0.72\n","Standard deviation f1-score = 0.029\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9142a3df"},"source":["## Model Task B"],"id":"9142a3df"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdvEEPmqN5E-","executionInfo":{"status":"ok","timestamp":1634104730322,"user_tz":420,"elapsed":10516351,"user":{"displayName":"Isabel G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12706972986805579164"}},"outputId":"2ae349a0-b70a-416e-c1be-b9927008be6b"},"source":["# initialize lists to keep statistics of all runs\n","f1_NGEN = []\n","f1_GEN = []\n","f1_macro_b = []\n","f1_weighted_b = []\n","\n","for i in range(15):\n","\n","  # delete model if exists\n","  try:\n","    del BERT_model_B\n","  except:\n","    pass\n","  \n","  # define the model. Task B is a classification task with 2 labels\n","  BERT_model_B = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","  # compile model\n","  BERT_model_B.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","                       loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                       metrics=tf.metrics.CategoricalAccuracy()\n","                       )\n","  \n","  # fit model\n","  training_history = BERT_model_B.fit(train_tf_dataset_b, validation_data=dev_tf_dataset_b, epochs=3)\n","\n","  print(f'---------------------------Iteration {i} ---------------------------\\n')\n","  # Evaluate model on TEST data\n","  # predict using model. Returns logits\n","  pred_labels_test = BERT_model_B.predict(test_features)[0]\n","\n","  # convert logits lo labels\n","  pred_labels_test = from_logits_to_labels(pred_labels_test, 'B')\n","\n","  # get f1-score for all classes, macro and weighted\n","  x = metrics.classification_report(test_labels_b, pred_labels_test, digits=3, output_dict=True)\n","  # append values to keep scores\n","  f1_NGEN.append(x['NGEN']['f1-score'])\n","  f1_GEN.append(x['GEN']['f1-score'])\n","  f1_macro_b.append(x['macro avg']['f1-score'])\n","  f1_weighted_b.append(x['weighted avg']['f1-score'])\n","\n","# calculate mean\n","f1_NGEN_mean = round(statistics.mean(f1_NGEN), 3)\n","f1_GEN_mean = round(statistics.mean(f1_GEN), 3)\n","f1_macro_b_mean = round(statistics.mean(f1_macro_b), 3)\n","f1_weighted_b_mean = round(statistics.mean(f1_weighted_b), 3)\n","\n","# calculate standard deviation\n","f1_NGEN_std = round(statistics.stdev(f1_NGEN), 3)\n","f1_GEN_std = round(statistics.stdev(f1_GEN), 3)\n","f1_macro_b_std = round(statistics.stdev(f1_macro_b), 3)\n","f1_weighted_b_std = round(statistics.stdev(f1_weighted_b), 3)\n","\n","print('Class NGEN')\n","print(f'Mean f1-score = {f1_NGEN_mean}')\n","print(f'Standard deviation f1-score = {f1_NGEN_std}\\n')\n","\n","print('Class GEN')\n","print(f'Mean f1-score = {f1_GEN_mean}')\n","print(f'Standard deviation f1-score = {f1_GEN_std}\\n')\n","\n","print('Macro')\n","print(f'Mean f1-score = {f1_macro_b_mean}')\n","print(f'Standard deviation f1-score = {f1_macro_b_std}\\n')\n","\n","print('Weighted')\n","print(f'Mean f1-score = {f1_weighted_b_mean}')\n","print(f'Standard deviation f1-score = {f1_weighted_b_std}\\n')\n"],"id":"qdvEEPmqN5E-","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.1980 - categorical_accuracy: 0.9301 - val_loss: 0.2133 - val_categorical_accuracy: 0.9447\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0398 - categorical_accuracy: 0.9884 - val_loss: 0.2813 - val_categorical_accuracy: 0.9390\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0199 - categorical_accuracy: 0.9938 - val_loss: 0.2840 - val_categorical_accuracy: 0.9418\n","---------------------------Iteration 0 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 219s 351ms/step - loss: 0.1821 - categorical_accuracy: 0.9317 - val_loss: 0.2041 - val_categorical_accuracy: 0.9353\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0375 - categorical_accuracy: 0.9888 - val_loss: 0.2753 - val_categorical_accuracy: 0.9465\n","Epoch 3/3\n","586/586 [==============================] - 203s 347ms/step - loss: 0.0286 - categorical_accuracy: 0.9913 - val_loss: 0.3954 - val_categorical_accuracy: 0.9456\n","---------------------------Iteration 1 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.1835 - categorical_accuracy: 0.9313 - val_loss: 0.2437 - val_categorical_accuracy: 0.9447\n","Epoch 2/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0278 - categorical_accuracy: 0.9921 - val_loss: 0.2846 - val_categorical_accuracy: 0.9409\n","Epoch 3/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0186 - categorical_accuracy: 0.9939 - val_loss: 0.3663 - val_categorical_accuracy: 0.9465\n","---------------------------Iteration 2 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.1800 - categorical_accuracy: 0.9332 - val_loss: 0.2605 - val_categorical_accuracy: 0.9278\n","Epoch 2/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0351 - categorical_accuracy: 0.9903 - val_loss: 0.2676 - val_categorical_accuracy: 0.9381\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0268 - categorical_accuracy: 0.9910 - val_loss: 0.2777 - val_categorical_accuracy: 0.9343\n","---------------------------Iteration 3 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.2439 - categorical_accuracy: 0.9071 - val_loss: 0.2221 - val_categorical_accuracy: 0.9203\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0468 - categorical_accuracy: 0.9865 - val_loss: 0.2900 - val_categorical_accuracy: 0.9437\n","Epoch 3/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0224 - categorical_accuracy: 0.9946 - val_loss: 0.3682 - val_categorical_accuracy: 0.9362\n","---------------------------Iteration 4 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.1729 - categorical_accuracy: 0.9361 - val_loss: 0.2393 - val_categorical_accuracy: 0.9437\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0263 - categorical_accuracy: 0.9916 - val_loss: 0.3136 - val_categorical_accuracy: 0.9268\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0189 - categorical_accuracy: 0.9936 - val_loss: 0.3818 - val_categorical_accuracy: 0.9428\n","---------------------------Iteration 5 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.1789 - categorical_accuracy: 0.9347 - val_loss: 0.2213 - val_categorical_accuracy: 0.9343\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0340 - categorical_accuracy: 0.9901 - val_loss: 0.2826 - val_categorical_accuracy: 0.9390\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0169 - categorical_accuracy: 0.9947 - val_loss: 0.4098 - val_categorical_accuracy: 0.9409\n","---------------------------Iteration 6 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.2045 - categorical_accuracy: 0.9239 - val_loss: 0.2764 - val_categorical_accuracy: 0.9371\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0510 - categorical_accuracy: 0.9852 - val_loss: 0.3136 - val_categorical_accuracy: 0.9250\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0167 - categorical_accuracy: 0.9956 - val_loss: 0.4152 - val_categorical_accuracy: 0.9212\n","---------------------------Iteration 7 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.2781 - categorical_accuracy: 0.8960 - val_loss: 0.2431 - val_categorical_accuracy: 0.9287\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0511 - categorical_accuracy: 0.9872 - val_loss: 0.2683 - val_categorical_accuracy: 0.9437\n","Epoch 3/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0262 - categorical_accuracy: 0.9932 - val_loss: 0.3213 - val_categorical_accuracy: 0.9325\n","---------------------------Iteration 8 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 353ms/step - loss: 0.1795 - categorical_accuracy: 0.9352 - val_loss: 0.3046 - val_categorical_accuracy: 0.9109\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0338 - categorical_accuracy: 0.9904 - val_loss: 0.2853 - val_categorical_accuracy: 0.9343\n","Epoch 3/3\n","586/586 [==============================] - 203s 347ms/step - loss: 0.0156 - categorical_accuracy: 0.9957 - val_loss: 0.2848 - val_categorical_accuracy: 0.9353\n","---------------------------Iteration 9 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 219s 351ms/step - loss: 0.2059 - categorical_accuracy: 0.9260 - val_loss: 0.2399 - val_categorical_accuracy: 0.9456\n","Epoch 2/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0537 - categorical_accuracy: 0.9835 - val_loss: 0.2603 - val_categorical_accuracy: 0.9493\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0403 - categorical_accuracy: 0.9888 - val_loss: 0.3490 - val_categorical_accuracy: 0.9428\n","---------------------------Iteration 10 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.2282 - categorical_accuracy: 0.9123 - val_loss: 0.2522 - val_categorical_accuracy: 0.9212\n","Epoch 2/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0423 - categorical_accuracy: 0.9883 - val_loss: 0.2936 - val_categorical_accuracy: 0.9400\n","Epoch 3/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0228 - categorical_accuracy: 0.9937 - val_loss: 0.3604 - val_categorical_accuracy: 0.9240\n","---------------------------Iteration 11 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 354ms/step - loss: 0.2649 - categorical_accuracy: 0.9042 - val_loss: 0.2140 - val_categorical_accuracy: 0.9437\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0692 - categorical_accuracy: 0.9815 - val_loss: 0.2509 - val_categorical_accuracy: 0.9353\n","Epoch 3/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0420 - categorical_accuracy: 0.9886 - val_loss: 0.3600 - val_categorical_accuracy: 0.9081\n","---------------------------Iteration 12 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 219s 351ms/step - loss: 0.1719 - categorical_accuracy: 0.9383 - val_loss: 0.2253 - val_categorical_accuracy: 0.9437\n","Epoch 2/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0321 - categorical_accuracy: 0.9903 - val_loss: 0.3452 - val_categorical_accuracy: 0.9447\n","Epoch 3/3\n","586/586 [==============================] - 203s 347ms/step - loss: 0.0255 - categorical_accuracy: 0.9926 - val_loss: 0.2628 - val_categorical_accuracy: 0.9371\n","---------------------------Iteration 13 ---------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","586/586 [==============================] - 220s 352ms/step - loss: 0.2143 - categorical_accuracy: 0.9194 - val_loss: 0.2375 - val_categorical_accuracy: 0.9334\n","Epoch 2/3\n","586/586 [==============================] - 204s 348ms/step - loss: 0.0334 - categorical_accuracy: 0.9908 - val_loss: 0.3107 - val_categorical_accuracy: 0.9428\n","Epoch 3/3\n","586/586 [==============================] - 204s 347ms/step - loss: 0.0207 - categorical_accuracy: 0.9946 - val_loss: 0.2036 - val_categorical_accuracy: 0.9325\n","---------------------------Iteration 14 ---------------------------\n","\n","Class NGEN\n","Mean f1-score = 0.911\n","Standard deviation f1-score = 0.011\n","\n","Class GEN\n","Mean f1-score = 0.519\n","Standard deviation f1-score = 0.035\n","\n","Macro\n","Mean f1-score = 0.715\n","Standard deviation f1-score = 0.019\n","\n","Weighted\n","Mean f1-score = 0.854\n","Standard deviation f1-score = 0.011\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"efcfaf92"},"source":["## References\n","\n","- Pre-processing data: https://huggingface.co/transformers/preprocessing.html\n","\n","- Fine-tunning a pre-trained model: https://huggingface.co/transformers/training.html\n","\n","- BERT: https://huggingface.co/transformers/model_doc/bert.html\n"],"id":"efcfaf92"},{"cell_type":"code","metadata":{"id":"ecf7c642"},"source":[""],"id":"ecf7c642","execution_count":null,"outputs":[]}]}