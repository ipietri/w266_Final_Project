{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "environment": {
      "name": "common-cu110.m80",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc-autonumbering": true,
    "colab": {
      "name": "Annotations-BoW Removed Irrelevant.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3a96351b-b72b-4d3a-930c-b1ced24420cf",
        "c6a1c3d5-4504-4f5f-abbb-59cd695d3cdb",
        "c5809fff-5919-44c5-9ddc-bbcc8817f859"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipietri/w266_Final_Project/blob/master/notebooks/RtGender-Notebooks/Removed_Irrelevant/Annotations_BoW_Removed_Irrelevant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e39fbde0"
      },
      "source": [
        "## RtGender- Annotations - BoW Neural Net - Removed Irrelevant\n",
        "---\n",
        "In this notebook we build the baseline models for the RtGender dataset. \n",
        "\n",
        "Characteristics of the models:\n",
        "- Neural Bag of Words architecture\n",
        "- A single dense layer with dropout\n",
        "- Use Glove embeddings (dim=300) without fine tuning them\n",
        "- Maximum sequence length is 100\n",
        "\n",
        "\n"
      ],
      "id": "e39fbde0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFZAqRZx8MGm",
        "outputId": "a4f1f2c6-b65a-4935-bc0b-f5a72d74f2e7"
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  path = r'/content/drive/MyDrive/w266'\n",
        "except ModuleNotFoundError:\n",
        "  path = r'data'"
      ],
      "id": "LFZAqRZx8MGm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10cd528a"
      },
      "source": [
        "## Package imports"
      ],
      "id": "10cd528a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY6lZmEA8Tc-"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_tuner"
      ],
      "id": "AY6lZmEA8Tc-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfb853e9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import tensorflow.keras.backend as K\n",
        "# for hyperparameter tunning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner import HyperModel\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# import sklearn to calculate the metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "id": "bfb853e9",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7762ba27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22466613-391e-4cf3-8b72-16281f776d8a"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "id": "7762ba27",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8da6ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f61cd23e-d5c3-44a3-84e7-cf4a98d78cd8"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "id": "b8da6ff7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7545875"
      },
      "source": [
        "# Load\n",
        "Load training, development and test datasets. See RtGender Split and Save notebook to see approach. "
      ],
      "id": "e7545875"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz1IKdoG5naN",
        "outputId": "e7625fd9-4c04-4b0c-baad-4efd6a6dc7e2"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/w266/annotations_train.csv') #train_oversampled\n",
        "dev_df = pd.read_csv('/content/drive/MyDrive/w266/annotations_dev.csv')\n",
        "\n",
        "print('train_shape: ',train_df.shape)\n",
        "print('dev_shape: ',dev_df.shape)"
      ],
      "id": "Dz1IKdoG5naN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_shape:  (10746, 9)\n",
            "dev_shape:  (2303, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeqCxEP5boxV",
        "outputId": "719a6f57-34d6-4a5a-dd74-327705035cc5"
      },
      "source": [
        "# remove irrelevant annotation examples\n",
        "train_df  = train_df[train_df['relevance'] != 'Irrelevant']\n",
        "dev_df = dev_df[dev_df['relevance'] != 'Irrelevant']\n",
        "\n",
        "print('updated train_shape: ',train_df.shape)\n",
        "print('updated dev_shape: ',dev_df.shape)"
      ],
      "id": "KeqCxEP5boxV",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated train_shape:  (9482, 9)\n",
            "updated dev_shape:  (2042, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drepZIvnAQPF",
        "outputId": "cd2b1ffa-a59d-4a9c-a874-be92bf169019"
      },
      "source": [
        "nan_values = dev_df[dev_df.isna().any(axis=1)] \n",
        "print(nan_values)\n",
        "\n",
        "# return without missing values in response_text\n",
        "dev_df.dropna(subset = [\"response_text\"], inplace=True)"
      ],
      "id": "drepZIvnAQPF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, source, op_gender, post_text, response_text, sentiment, relevance, label, labels_4]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDdFMAl57ztl"
      },
      "source": [
        "X_train = train_df['response_text']\n",
        "y_train = train_df['labels_4']\n",
        "X_dev = dev_df['response_text']\n",
        "y_dev = dev_df['labels_4']"
      ],
      "id": "mDdFMAl57ztl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40c4ca26"
      },
      "source": [
        "## Load GloVe "
      ],
      "id": "40c4ca26"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e43db36"
      },
      "source": [
        "# load pre-trained word embeddings. In this case Glove\n",
        "# This is commented out to avoid downloading it again\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -P ~/data/\n",
        "\n",
        "# # unzip the file\n",
        "# # commented out for the same reason above\n",
        "!unzip ~/data/glove.6B.zip -d ~/data/"
      ],
      "id": "8e43db36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8167680e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f9b911-63e1-419a-b2ab-30ab1197e371"
      },
      "source": [
        "# path to glove file- will use the embeddings with dimension = 300\n",
        "glove_file =\"/root/data/glove.6B.300d.txt\"\n",
        "\n",
        "# embedding layer \n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_file))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "id": "8167680e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0798594a"
      },
      "source": [
        "# Preprocess"
      ],
      "id": "0798594a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6da2e52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe3b039-f2c0-4222-b754-e59cd2903c96"
      },
      "source": [
        "# tokenize data and return embeddings matrix \n",
        "max_tokens = 10000\n",
        "# consider this maximum number of words\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(num_words=max_tokens)\n",
        "\n",
        "#x_train, x_dev, embedding_matrix = tokenize_Xtrain_and_Xdev(X_train, X_dev)\n",
        "\n",
        "# one-hot encodign and reshape labels\n",
        "print(\"-\"*60)\n",
        "\n",
        "train_labels = to_categorical(np.asarray(y_train))\n",
        "dev_labels = to_categorical(np.asarray(y_dev))\n",
        "\n",
        "print('Shape of train label tensor:', train_labels.shape)\n",
        "print('Shape of dev label tensor:', dev_labels.shape)"
      ],
      "id": "6da2e52d",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "Shape of train label tensor: (9482, 4)\n",
            "Shape of dev label tensor: (2042, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11e15aa"
      },
      "source": [
        "## Preprocess & Embeddings matrix"
      ],
      "id": "a11e15aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDqFAiucCFLw",
        "outputId": "e855a67c-ef4f-45a6-b18c-4d8762abd966"
      },
      "source": [
        "x_train = X_train \n",
        "x_dev = X_dev\n",
        " # note the length of the training index\n",
        "train_idx = len(x_train)\n",
        "#combine train and dev data and then tokenize\n",
        "texts = x_train.append(x_dev)\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Train + Dev %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "\n",
        "x_train = data[:train_idx]\n",
        "x_dev = data[train_idx:]\n",
        "print('Shape of train data tensor:', x_train.shape)\n",
        "print('Shape of dev data tensor:', x_dev.shape)\n",
        "print(\"-\"*60)\n",
        "\n",
        "# print top 5 most and least common tokens\n",
        "print(\"top 5 most common tokens: \", sorted(word_index, key=word_index.get, reverse=True)[-5:])\n",
        "print(\"top 5 least common tokens: \", sorted(word_index, key=word_index.get, reverse=True)[:5])\n",
        "print(\"-\"*60)\n",
        "\n",
        "# build embedding matrix to use it in the model\n",
        "dimensions_emb = 300\n",
        "\n",
        "# zero position is zero by default in keras\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, dimensions_emb)) \n",
        "\n",
        "total_tokens = len(word_index) + 1\n",
        "with_embedding = []\n",
        "without_embedding = []\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        with_embedding.append(word)\n",
        "    else:\n",
        "        without_embedding.append(word)\n",
        "\n",
        "print(f'Number of words with embeddings found: {len(with_embedding)}')\n",
        "print(f'Number of words with embeddings NOT found: {len(without_embedding)}')        \n",
        "\n"
      ],
      "id": "iDqFAiucCFLw",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train + Dev 18096 unique tokens.\n",
            "Shape of data tensor: (11524, 100)\n",
            "Shape of train data tensor: (9482, 100)\n",
            "Shape of dev data tensor: (2042, 100)\n",
            "------------------------------------------------------------\n",
            "top 5 most common tokens:  ['and', 'you', 'i', 'to', 'the']\n",
            "top 5 least common tokens:  ['grandeur', 'inadequate', 'woefully', 'lodge', 'jazzy']\n",
            "------------------------------------------------------------\n",
            "Number of words with embeddings found: 15510\n",
            "Number of words with embeddings NOT found: 2586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f1b2d4-0d72-4040-812c-5391371ecb36"
      },
      "source": [
        "## Metrics"
      ],
      "id": "09f1b2d4-0d72-4040-812c-5391371ecb36"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eee790a"
      },
      "source": [
        "def binary_loss_accuracy_plots(training_history):\n",
        "    '''\n",
        "    Returns plots for loss and accuracy during the training process of a NN.\n",
        "    Parameters:\n",
        "    training_history: object that stores the training history of the NN (from model.fit(...))\n",
        "    xrange: range in x axis\n",
        "    task: string used for the title in the plot\n",
        "    '''\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "    \n",
        "    # summarize history for accuracy\n",
        "    ax1.plot(training_history.history['binary_accuracy'])\n",
        "    ax1.plot(training_history.history['val_binary_accuracy'])\n",
        "    ax1.set_title('model accuracy')\n",
        "    ax1.set_ylabel('accuracy')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.legend(['train', 'dev'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    ax2.plot(training_history.history['loss'])\n",
        "    ax2.plot(training_history.history['val_loss'])\n",
        "    ax2.set_title('model loss')\n",
        "    ax2.set_ylabel('loss')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.legend(['train', 'dev'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def multiclass_loss_accuracy_plots(training_history):\n",
        "    '''\n",
        "    Returns plots for loss and accuracy during the training process of a NN.\n",
        "    Parameters:\n",
        "    training_history: object that stores the training history of the NN (from model.fit(...))\n",
        "    '''\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "    \n",
        "    # summarize history for accuracy\n",
        "    ax1.plot(training_history.history['categorical_accuracy'])\n",
        "    ax1.plot(training_history.history['val_categorical_accuracy'])\n",
        "    ax1.set_title('model accuracy')\n",
        "    ax1.set_ylabel('accuracy')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.legend(['train', 'dev'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    ax2.plot(training_history.history['loss'])\n",
        "    ax2.plot(training_history.history['val_loss'])\n",
        "    ax2.set_title('model loss')\n",
        "    ax2.set_ylabel('loss')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.legend(['train', 'dev'], loc='upper left')\n",
        "\n",
        "    plt.show()"
      ],
      "id": "5eee790a",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db73c525"
      },
      "source": [
        "def confusion_matrix(x_dev, original_dev_labels, class_labels):\n",
        "\n",
        "    # identify the correct class\n",
        "    max_class_idx = np.argmax(y_pred, axis = 1)\n",
        "    \n",
        "    # Create a confusion matrix\n",
        "    cm = tf.math.confusion_matrix(original_dev_labels, max_class_idx)\n",
        "    cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm, annot=True,\n",
        "        xticklabels=class_labels,\n",
        "        yticklabels=class_labels,\n",
        "        fmt='.2%', cmap='Blues')\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "    \n",
        "    # print classification report\n",
        "    print(metrics.classification_report(original_dev_labels, max_class_idx))\n",
        "    # print global weighted f-1 score\n",
        "    f1 = metrics.f1_score(original_dev_labels, max_class_idx, pos_label=1, average='weighted')*100\n",
        "    print(f\"Weighted f1-score: %.2f%%\" %f1)\n"
      ],
      "id": "db73c525",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ed5OcoqDAs0"
      },
      "source": [
        "# Optimal Model Iterated"
      ],
      "id": "4Ed5OcoqDAs0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LQchzGjNh_"
      },
      "source": [
        "See RtGender_Annotations_Removed_Irrelevant_BoW_Neural_Net for optimal model hyperparam run\n",
        "\n",
        "\"The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is 40 and the optimal learning rate for the optimizer\n",
        "is 0.001.\""
      ],
      "id": "Q9LQchzGjNh_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kc8bVqcE1Ig"
      },
      "source": [
        "iterations = 5\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                            embedding_matrix.shape[1],\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(embedding_layer) \n",
        "model.add(tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1))) #avg\n",
        "\n",
        "# Tune the number of units in the first Dense layer\n",
        "# Choose an optimal value between 10 and 100\n",
        "model.add(tf.keras.layers.Dense(units=40, activation='relu'))\n",
        "\n",
        "   \n",
        "# output layer \n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Tune the learning rate for the optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "            loss='categorical_crossentropy', \n",
        "            metrics=[tf.keras.metrics.categorical_accuracy, 'categorical_crossentropy'])"
      ],
      "id": "_Kc8bVqcE1Ig",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-eLc5pbDLmo",
        "outputId": "a66ef305-1432-4c9f-c01a-36983f7d6af5"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "accuracy_list = []\n",
        "weighted_f1_score_list = []\n",
        "macro_f1_score_list = []\n",
        "negative_f1_score = []\n",
        "neutral_f1_score = []\n",
        "mixed_f1_score = []\n",
        "positive_f1_score = []\n",
        "\n",
        "for i in range(iterations):\n",
        "  try:\n",
        "    del history\n",
        "    del y_pred\n",
        "    del cr\n",
        "  except: pass\n",
        "\n",
        "\n",
        "  history = reconstructed_model.fit(x_train, train_labels,\n",
        "                      validation_data=(x_dev, dev_labels),\n",
        "                      epochs=50, verbose=0)\n",
        "  y_pred = reconstructed_model.predict(x_dev) #predict\n",
        "\n",
        "  # metrics append to appropriate lists\n",
        "  predictions = y_pred.argmax(1)\n",
        "  cr = metrics.classification_report(y_dev, predictions, digits=3, output_dict=True)\n",
        "  \n",
        "  accuracy_list.append(cr.get('accuracy'))\n",
        "  weighted_f1_score_list.append(cr.get('weighted avg').get(\"f1-score\"))\n",
        "  macro_f1_score_list.append(cr.get(\"macro avg\").get(\"f1-score\"))\n",
        "\n",
        "  negative_f1_score.append(cr.get('0').get(\"f1-score\"))\n",
        "  neutral_f1_score.append(cr.get('1').get(\"f1-score\"))\n",
        "  positive_f1_score.append(cr.get('2').get(\"f1-score\"))\n",
        "  mixed_f1_score.append(cr.get('3').get(\"f1-score\"))\n",
        "\n",
        "  print(f'---------------------------Iteration {i+1} Complete---------------------------\\n')\n"
      ],
      "id": "n-eLc5pbDLmo",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------Iteration 1 Complete---------------------------\n",
            "\n",
            "---------------------------Iteration 2 Complete---------------------------\n",
            "\n",
            "---------------------------Iteration 3 Complete---------------------------\n",
            "\n",
            "---------------------------Iteration 4 Complete---------------------------\n",
            "\n",
            "---------------------------Iteration 5 Complete---------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEFb5Ti0FBth",
        "outputId": "830236d0-636b-4c1c-cb56-4a93527ab857"
      },
      "source": [
        "import statistics\n",
        "\n",
        "print(\"%15s %s (%s)\" % (\"\",\"Mean\", \"StDev\"))\n",
        "\n",
        "print(\"-\"*29)\n",
        "print(\"Macro Scores\")\n",
        "print(\"-\"*29)\n",
        "\n",
        "print(f\"%15s %s (%s)\" %(\"Accuracy\",\n",
        "    round(statistics.mean(accuracy_list),3),\n",
        "    round(statistics.stdev(accuracy_list),3)))\n",
        "print(f\"%15s %5s (%s)\" %(\"Macro F1\",\n",
        "    round(statistics.mean(macro_f1_score_list),3),\n",
        "    round(statistics.stdev(macro_f1_score_list),3)))\n",
        "print(f\"%15s %5s (%s)\" %(\"Weighted F1\",\n",
        "    round(statistics.mean(weighted_f1_score_list),3),\n",
        "    round(statistics.stdev(weighted_f1_score_list),3)))\n",
        "\n",
        "print(\"-\"*29)\n",
        "print(\"Class Scores\")\n",
        "print(\"-\"*29)\n",
        "\n",
        "print(f\"%15s %s (%s)\" %(\"Positive\",\n",
        "    round(statistics.mean(positive_f1_score),3),\n",
        "    round(statistics.stdev(positive_f1_score),3)))\n",
        "print(f\"%15s %5s (%s)\" %(\"Neutral\",\n",
        "    round(statistics.mean(neutral_f1_score),3),\n",
        "    round(statistics.stdev(neutral_f1_score),3)))\n",
        "print(f\"%15s %5s (%s)\" %(\"Negative\",\n",
        "    round(statistics.mean(negative_f1_score),3),\n",
        "    round(statistics.stdev(negative_f1_score),3)))\n",
        "print(f\"%15s %5s (%s)\" %(\"Mixed\",\n",
        "    round(statistics.mean(mixed_f1_score),3),\n",
        "    round(statistics.stdev(mixed_f1_score),3)))"
      ],
      "id": "cEFb5Ti0FBth",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Mean (StDev)\n",
            "-----------------------------\n",
            "Macro Scores\n",
            "-----------------------------\n",
            "       Accuracy 0.493 (0.007)\n",
            "       Macro F1 0.328 (0.007)\n",
            "    Weighted F1 0.467 (0.002)\n",
            "-----------------------------\n",
            "Class Scores\n",
            "-----------------------------\n",
            "       Positive 0.687 (0.008)\n",
            "        Neutral 0.249 (0.015)\n",
            "       Negative 0.254 (0.017)\n",
            "          Mixed 0.122 (0.016)\n"
          ]
        }
      ]
    }
  ]
}