{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grey_scale_augmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvwv73zh0CNHtnwvCeVg9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipietri/w266_Final_Project/blob/master/notebooks/RtGender-Notebooks/grey_scale_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcifIzYl3C5"
      },
      "source": [
        "# For Greyscaling Data Augmentation \n",
        "\n",
        "References:\n",
        "[Code Source](https://github.com/ainagari/scalar_adjs)\n",
        "\n",
        "* [BERT Knows Punta Cana is not just beautiful, itâ€™s gorgeous:\n",
        "Ranking Scalar Adjectives with Contextualised Representations](https://aclanthology.org/2020.emnlp-main.598.pdf)\\\n",
        "*[Scalar Adjective Identification and Multilingual Ranking\n",
        "](https://arxiv.org/abs/2105.01180)\\\n",
        "*[Identifying and Ordering Scalar Adjectives Using Lexical Substitution](https://www.proquest.com/openview/aade435a5bbdcf41e2b8c24e648826cc/1.pdf?pq-origsite=gscholar&cbl=18750)\\\n",
        "*[A Gold Standard for Scalar Adjectives](https://aclanthology.org/L16-1424/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nVvQNOKmO0G"
      },
      "source": [
        "import copy\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertModel, AutoTokenizer, AutoModel, FlaubertTokenizer, FlaubertModel, AutoConfig, FlaubertConfig\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV7rTQDEnk61"
      },
      "source": [
        "language_str = \"en\"\n",
        "#whether we exclude the last bpe of words when words are split into multiple wordpieces\n",
        "exclude_last_bpe =\"True\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbk_YXtTlu96"
      },
      "source": [
        "# from extract_representations.py\n",
        "def special_tokenization(sentence, tokenizer, model_name):\n",
        "    map_ori_to_bert = []\n",
        "    if \"flaubert\" in model_name:\n",
        "        tok_sent = ['<s>']\n",
        "    else:\n",
        "        tok_sent = ['[CLS]']\n",
        "\n",
        "    for orig_token in sentence.split():\n",
        "        current_tokens_bert_idx = [len(tok_sent)]\n",
        "        bert_token = tokenizer.tokenize(orig_token) # tokenize\n",
        "        tok_sent.extend(bert_token) # add to my new tokens\n",
        "        if len(bert_token) > 1: # if the new token has been 'wordpieced'\n",
        "            extra = len(bert_token) - 1\n",
        "            for i in range(extra):\n",
        "                current_tokens_bert_idx.append(current_tokens_bert_idx[-1]+1) # list of new positions of the target word in the new tokenization\n",
        "        map_ori_to_bert.append(tuple(current_tokens_bert_idx))\n",
        "\n",
        "    if \"flaubert\" in model_name:\n",
        "        tok_sent.append('</s>')\n",
        "    else:\n",
        "        tok_sent.append('[SEP]')\n",
        "\n",
        "    return tok_sent, map_ori_to_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRM5K3BSmTOl"
      },
      "source": [
        "## Augment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0NLFTrxmDGG"
      },
      "source": [
        "infos = []\n",
        "final_dict = []\n",
        "labels_list = []\n",
        "augmented_text_list = []\n",
        "\n",
        "def augment_greyscaling(dict_from_df, datanames, text_col, scale_col):\n",
        "  '''return augmented text with milder words from selected scales'''\n",
        "  for id, values in dict_from_df.items():\n",
        "    num_positions = 0\n",
        "    for data_name in datanames:\n",
        "      # sum the number of word positions for replacement\n",
        "      num_positions += len(dict_from_df[id]['new_col'][data_name])\n",
        "\n",
        "      if num_positions == 0:\n",
        "        # if none begin again\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        for word, values in dict_from_df[id]['new_col'][data_name].items():\n",
        "          num_mild_words = len(dict_from_df[id]['new_col'][data_name][word]['milder_words'])\n",
        "          \n",
        "          if num_mild_words == 0:\n",
        "            # if there are no milder words in the scale move to next word\n",
        "            continue\n",
        "\n",
        "          else:\n",
        "            # assume the word is only in one location in the example text\n",
        "            position_scaleword = dict_from_df[id]['new_col'][data_name][word]['position']\n",
        "            cinstance = dict()\n",
        "            cinstance[text_col] = copy.deepcopy(dict_from_df[id][text_col])\n",
        "\n",
        "            # convert text to a list\n",
        "            sentence_words = dict_from_df[id][text_col].replace(\"'\", \"\") \n",
        "            sentence_words = sentence_words.split(\" \")\n",
        "            for scale_word in dict_from_df[id]['new_col'][data_name][word]['milder_words']:\n",
        "\n",
        "              # change a to an and vice versa depending on first letter of the scaleword \n",
        "              if sentence_words[position_scaleword-1] == \"a\" and scale_word[0] in \"aeiou\":\n",
        "                sentence_words[position_scaleword-1] = \"an\"\n",
        "              elif sentence_words[position_scaleword-1] == \"an\" and scale_word[0] not in \"aeiou\":\n",
        "                sentence_words[position_scaleword-1] = \"a\"\n",
        "              \n",
        "              # and replace the scaleword\n",
        "              sentence_words[position_scaleword] = scale_word\n",
        "              cinstance[\"position\"] = [position_scaleword]\n",
        "              \n",
        "              # extract and tokenize the original sentence\n",
        "              example = ' '.join(sentence_words)\n",
        "\n",
        "              # add augmented text to final dictionary\n",
        "              test_df.loc[len(test_df)] = [dict_from_df[id][scale_col], \n",
        "                                          ' '.join(sentence_words)]\n",
        "              labels_list.append(dict_from_df[id][scale_col])\n",
        "              augmented_text_list.append(' '.join(sentence_words))\n",
        "\n",
        "              bert_tokenized_sentence, mapp = special_tokenization(example, tokenizer, model_name)\n",
        "              \n",
        "              current_positions = cinstance['position']\n",
        "              if len(current_positions) == 1:\n",
        "                  bert_position = mapp[cinstance['position'][0]] \n",
        "              elif len(current_positions) > 1:\n",
        "                bert_position = []\n",
        "                for p in current_positions:\n",
        "                    bert_position.extend(mapp[p])\n",
        "              \n",
        "              cinstance[id] = id\n",
        "              cinstance[\"bert_tokenized_sentence\"] = bert_tokenized_sentence\n",
        "              cinstance[\"bert_position\"] = bert_position\n",
        "              cinstance[\"scale\"] = scale\n",
        "              cinstance[\"lemma\"] = scale_word\n",
        "              infos.append(cinstance)\n",
        "\n",
        "  return labels_list, augmented_text_list, infos, final_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}