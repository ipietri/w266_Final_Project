{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20c919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pyarrow as pa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec5612",
   "metadata": {},
   "source": [
    "## Initial Hugging Face Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the first time\n",
    "#!pip install datasets\n",
    "#!python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\"\n",
    "#!git clone https://github.com/huggingface/datasets.git\n",
    "#!pip install -e datasets/ \n",
    "#!python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the first time\n",
    "# from datasets import get_dataset_config_names\n",
    "\n",
    "# configs = get_dataset_config_names('peixian/rtGender')\n",
    "# print(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620870bb",
   "metadata": {},
   "source": [
    "# Load Data from Hugging Face\n",
    "[config info](https://huggingface.co/datasets/peixian/rtGender/blob/main/rtGender.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90841f9f",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Split on Posts to ensure we have target number of posts.. \\\n",
    "#TODO: check whether there is any imbalance in source or labels for these splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2027792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset rt_gender (/home/kbaumberg/.cache/huggingface/datasets/rt_gender/posts/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1)\n",
      "Loading cached split indices for dataset at /home/kbaumberg/.cache/huggingface/datasets/rt_gender/posts/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1/cache-bd5ec9e8afb99b93.arrow and /home/kbaumberg/.cache/huggingface/datasets/rt_gender/posts/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1/cache-9d2608a1526a2fac.arrow\n",
      "Loading cached split indices for dataset at /home/kbaumberg/.cache/huggingface/datasets/rt_gender/posts/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1/cache-94833b73cb653da6.arrow and /home/kbaumberg/.cache/huggingface/datasets/rt_gender/posts/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1/cache-e0fab6ad72eb1756.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'op_id', 'op_gender', 'post_id', 'post_text', 'post_type', 'subreddit', 'op_gender_visible'],\n",
      "    num_rows: 2469836\n",
      "})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DatasetDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-237cb68590c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_devtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mposts_dev_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_devtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m posts_train_dev_test_dataset = DatasetDict({\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_devtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mposts_dev_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetDict' is not defined"
     ]
    }
   ],
   "source": [
    "posts = load_dataset('peixian/rtGender', 'posts', split = 'train')\n",
    "print(posts)\n",
    "posts[0]\n",
    "\n",
    "# save out a list of unique post_ids for the training and then dev sets\n",
    "# split train 70%/ dev 15% / test 15%\n",
    "\n",
    "# split twice and combine\n",
    "train_devtest = posts.train_test_split(shuffle = True, seed = 200, test_size=0.3)\n",
    "posts_dev_test = train_devtest['test'].train_test_split(shuffle = True, seed = 200, test_size=0.50)\n",
    "posts_train_dev_test_dataset = DatasetDict({\n",
    "    'train': train_devtest['train'],\n",
    "    'test': posts_dev_test['test'],\n",
    "    'dev': posts_dev_test['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f74980",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_id_train_list = list(set(train_devtest['train']['post_id']))\n",
    "post_id_dev_list = list(set(posts_dev_test['train']['post_id']))\n",
    "post_id_test_list = list(set(posts_dev_test['test']['post_id']))\n",
    "\n",
    "# save out \n",
    "import csv\n",
    "\n",
    "with open('post_ids_train', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(post_id_train_list)\n",
    "    \n",
    "\n",
    "with open('post_ids_dev', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(post_id_dev_list)    \n",
    "\n",
    "with open('post_ids_test', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(post_id_test_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9674474",
   "metadata": {},
   "source": [
    "## Add and filter Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ee535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset rt_gender (/home/kbaumberg/.cache/huggingface/datasets/rt_gender/responses/1.1.0/62d8f34778a4eda10ed650d804cbb7ad4da391ac28f38034f424a2e9a7fc43d1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'op_id', 'op_gender', 'post_id', 'responder_id', 'response_text', 'op_name', 'op_category', 'responder_gender', 'responder_gender_visible', 'subreddit'],\n",
      "    num_rows: 26496468\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': '/home/kbaumberg/.cache/huggingface/datasets/downloads/extracted/2a2b549303f3c7cbf8b1989705b31377e104d11b1a2af73c46c2c726e0b54b3a/facebook_congress_responses',\n",
       " 'op_id': '57265377',\n",
       " 'op_gender': 'M',\n",
       " 'post_id': '0000000',\n",
       " 'responder_id': 'Jerry',\n",
       " 'response_text': 'Protecting birth is not the same as protecting life. You may very well pledge to the former but you fail miserably at the latter.',\n",
       " 'op_name': 'Roger Williams',\n",
       " 'op_category': 'Congress_Republican',\n",
       " 'responder_gender': None,\n",
       " 'responder_gender_visible': None,\n",
       " 'subreddit': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = load_dataset('peixian/rtGender', 'responses', split = 'train')\n",
    "print(responses)\n",
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea47cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ae4f4f2b8d4c1586d09384cead3cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26497 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'isin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6c5b75d41a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponses_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_id_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresponses_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         }\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameter `remove_columns` passed to .filter() is no longer supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2173\u001b[0;31m         indices = self.map(\n\u001b[0m\u001b[1;32m   2174\u001b[0m             \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_indices_from_mask_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   1689\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         }\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2049\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   2050\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0meffective_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1941\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1942\u001b[0m             )\n\u001b[1;32m   1943\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final_project/w266_Final_Project/notebooks/datasets/src/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, input_columns, *args)\u001b[0m\n\u001b[1;32m   3377\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m                 \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3379\u001b[0;31m                 \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3380\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;31m# inputs is a list of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6c5b75d41a84>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(example)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponses_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_id_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresponses_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'isin'"
     ]
    }
   ],
   "source": [
    "responses_test = responses.filter(lambda example: example['post_id'].isin(post_id_test_list))\n",
    "responses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_train = responses.filter(lambda example: example['post_id'] in post_id_train_list)\n",
    "responses_train\n",
    "\n",
    "responses_dev = responses.filter(lambda example: example['post_id'] in post_id_dev_list)\n",
    "responses_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af83f87",
   "metadata": {},
   "source": [
    "# Add Annotations\n",
    "\"30k annotations for the sentiment and relevance of these responses, showing that across our datasets responses to women are more likely to be emotive and about the speaker as an individual (rather than about the content being responded to)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = load_dataset('peixian/rtGender', 'annotations', split = 'train')\n",
    "print(annotations)\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b9f78",
   "metadata": {},
   "source": [
    "## Confirm the data are the same as in the EDA where we manually added the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76af801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA notes 15352 records in annotations. Confirmed here\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the counts for each data source\n",
    "# confirmed these counts match the EDA as well. We can use this dataset for our analysis    \n",
    "\n",
    "Counter(annotations['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gender split: \", Counter(annotations['op_gender']))\n",
    "print(\"sentiment split: \", Counter(annotations['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Arrow table to pandas df \n",
    "# https://arrow.apache.org/docs/python/pandas.html\n",
    "df_annotations = annotations.to_pandas()\n",
    "df_annotations.groupby(['source', 'op_gender']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13d96",
   "metadata": {},
   "source": [
    "Fairly even distribution by soure. No sampling required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78073510",
   "metadata": {},
   "source": [
    "# PARKING LOT\n",
    "\n",
    "RtGender Data Loading Into Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the first time you run this notebook\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5068a70",
   "metadata": {},
   "source": [
    "[Source code](https://www.kaggle.com/nkaenzig/bert-tensorflow-2-huggingface-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf import keras\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343df8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train = load_dataset('peixian/rtGender', 'posts', split = 'train[:900]')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "dataset = response_train.map(lambda e: tokenizer(e['post_text'], truncation=True, padding='max_length'), batched=True)\n",
    "dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'op_gender'])\n",
    "features = {x: dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[\"op_gender\"])).batch(32)\n",
    "next(iter(tfdataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_valid = load_dataset('peixian/rtGender', 'posts', split = 'train[901:1000]')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
